{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# 循环神经网络（RNN）\n","这里，我们使用 RNN 来进行尼采风格文本的自动生成。 \n","这个任务的本质其实预测一段英文文本的接续字母的概率分布。比如，我们有以下句子:\n","\n","```\n","I am a studen\n","```\n","\n","这个句子（序列）一共有 13 个字符（包含空格）。当我们阅读到这个由 13 个字符组成的序列后，根据我们的经验，我们可以预测出下一个字符很大概率是 “t”。我们希望建立这样一个模型，逐个输入一段长为 `seq_length` 的序列，输出这些序列接续的下一个字符的概率分布。我们从下一个字符的概率分布中采样作为预测值，然后滚雪球式地生成下两个字符，下三个字符等等，即可完成文本的生成任务。\n","\n","首先，还是实现一个简单的 `DataLoader` 类来读取文本，并以字符为单位进行编码。设字符种类数为 `num_chars` ，则每种字符赋予一个 0 到 `num_chars - 1` 之间的唯一整数编号 i。"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["class DataLoader():\n","    def __init__(self):\n","        path = tf.keras.utils.get_file('nietzsche.txt',\n","            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n","        with open(path, encoding='utf-8') as f:\n","            self.raw_text = f.read().lower()\n","        self.chars = sorted(list(set(self.raw_text)))\n","        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n","        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n","        self.text = [self.char_indices[c] for c in self.raw_text]\n","\n","    def get_batch(self, seq_length, batch_size):\n","        seq = []\n","        next_char = []\n","        for i in range(batch_size):\n","            index = np.random.randint(0, len(self.text) - seq_length)\n","            seq.append(self.text[index:index+seq_length])\n","            next_char.append(self.text[index+seq_length])\n","        return np.array(seq), np.array(next_char)       # [batch_size, seq_length], [num_batch]"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["接下来进行模型的实现。在 `__init__` 方法中我们实例化一个常用的 `LSTMCell` 单元，以及一个线性变换用的全连接层，我们首先对序列进行 “One Hot” 操作，即将序列中的每个字符的编码 i 均变换为一个 `num_char` 维向量，其第 i 位为 1，其余均为 0。变换后的序列张量形状为 `[seq_length, num_chars]` 。然后，我们初始化 RNN 单元的状态，存入变量 `state` 中。接下来，将序列从头到尾依次送入 RNN 单元，即在 t 时刻，将上一个时刻 t-1 的 RNN 单元状态 `state` 和序列的第 t 个元素 `inputs[t, :]` 送入 RNN 单元，得到当前时刻的输出 `output` 和 RNN 单元状态。取 RNN 单元最后一次的输出，通过全连接层变换到 `num_chars` 维，即作为模型的输出。"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["class RNN(tf.keras.Model):\n","    def __init__(self, num_chars, batch_size, seq_length):\n","        super().__init__()\n","        self.num_chars = num_chars\n","        self.seq_length = seq_length\n","        self.batch_size = batch_size\n","        self.cell = tf.keras.layers.LSTMCell(units=256)\n","        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n","\n","    def call(self, inputs, from_logits=False):\n","        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]\n","        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)\n","        for t in range(self.seq_length):\n","            output, state = self.cell(inputs[:, t, :], state)\n","        logits = self.dense(output)\n","        if from_logits:\n","            return logits\n","        else:\n","            return tf.nn.softmax(logits)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# 设置超参数\n","num_batches = 1000\n","seq_length = 40\n","batch_size = 50\n","learning_rate = 1e-3"]},{"cell_type":"markdown","execution_count":5,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-5-8d9875869dff>, line 1)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-8d9875869dff>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    - 从 `DataLoader` 中随机取一批训练数据；\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["- 从 `DataLoader` 中随机取一批训练数据；\n","- 将这批数据送入模型，计算出模型的预测值；\n","- 将模型预测值与真实值进行比较，计算损失函数（loss）；\n","- 计算损失函数关于模型变量的导数；\n","- 使用优化器更新模型参数以最小化损失函数。"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["data_loader = DataLoader()\n","model = RNN(num_chars=len(data_loader.chars), batch_size=batch_size, seq_length=seq_length)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","for batch_index in range(num_batches):\n","    X, y = data_loader.get_batch(seq_length, batch_size)\n","    with tf.GradientTape() as tape:\n","        y_pred = model(X)\n","        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n","        loss = tf.reduce_mean(loss)\n","        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n","    grads = tape.gradient(loss, model.variables)\n","    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["关于文本生成的过程有一点需要特别注意。之前，我们一直使用 `tf.argmax()` 函数，将对应概率最大的值作为预测值。然而对于文本生成而言，这样的预测方式过于绝对，会使得生成的文本失去丰富性。于是，我们使用 `np.random.choice()` 函数按照生成的概率分布取样。这样，即使是对应概率较小的字符，也有机会被取样到。同时，我们加入一个 `temperature` 参数控制分布的形状，参数值越大则分布越平缓（最大值和最小值的差值越小），生成文本的丰富度越高；参数值越小则分布越陡峭，生成文本的丰富度越低"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def predict(self, inputs, temperature=1.):\n","    batch_size, _ = tf.shape(inputs)\n","    logits = self(inputs, from_logits=True)\n","    prob = tf.nn.softmax(logits / temperature).numpy()\n","    return np.array([np.random.choice(self.num_chars, p=prob[i, :])\n","                         for i in range(batch_size.numpy())])"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["X_, _ = data_loader.get_batch(seq_length, 1)\n","for diversity in [0.2, 0.5, 1.0, 1.2]:\n","    X = X_\n","    print(\"diversity %f:\" % diversity)\n","    for t in range(400):\n","        y_pred = model.predict(X, diversity)\n","        print(data_loader.indices_char[y_pred[0]], end='', flush=True)\n","        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
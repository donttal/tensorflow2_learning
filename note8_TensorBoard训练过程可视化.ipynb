{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"TensorBoard 就是一个能够帮助我们将训练过程可视化的工具。\n\n首先在代码目录下建立一个文件夹（如 `./tensorboard` ）存放 TensorBoard 的记录文件，并在代码中实例化一个记录器："},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"summary_writer = tf.summary.create_file_writer('./tensorboard')     # 参数为记录文件所保存的目录"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"接下来，当需要记录训练过程中的参数时，通过 with 语句指定希望使用的记录器，并对需要记录的参数（一般是 scalar）运行 `tf.summary.scalar(name, tensor, step=batch_index)` ，即可将训练过程中参数在 step 时候的值记录下来。这里的 step 参数可根据自己的需要自行制定，一般可设置为当前训练过程中的 batch 序号。整体框架如下："},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"summary_writer = tf.summary.create_file_writer('./tensorboard')\n# 开始模型训练\nfor batch_index in range(num_batches):\n    # ...（训练代码，当前batch的损失值放入变量loss中）\n    with summary_writer.as_default():                               # 希望使用的记录器\n        tf.summary.scalar(\"loss\", loss, step=batch_index)\n        tf.summary.scalar(\"MyScalar\", my_scalar, step=batch_index)  # 还可以添加其他自定义的变量"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"每运行一次 `tf.summary.scalar()` ，记录器就会向记录文件中写入一条记录。除了最简单的标量（scalar）以外，TensorBoard 还可以对其他类型的数据（如图像，音频等）进行可视化，详见 [TensorBoard 文档](https://www.tensorflow.org/tensorboard/r2/get_started) 。\n\n当我们要对训练过程可视化时，在代码目录打开终端（如需要的话进入 TensorFlow 的 conda 环境），运行:"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"tensorboard --logdir=./tensorboard"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"然后使用浏览器访问命令行程序所输出的网址（一般是 http:// 计算机名称：6006），即可访问 TensorBoard 的可视界面，如下图所示：\n\n[![../../_images/tensorboard.png](https://tf.wiki/_images/tensorboard.png)](https://tf.wiki/_images/tensorboard.png)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"默认情况下，TensorBoard 每 30 秒更新一次数据。不过也可以点击右上角的刷新按钮手动刷新。\n\nTensorBoard 的使用有以下注意事项：\n\n- 如果需要重新训练，需要删除掉记录文件夹内的信息并重启 TensorBoard（或者建立一个新的记录文件夹并开启 TensorBoard， `--logdir` 参数设置为新建立的文件夹）；\n- 记录文件夹目录保持全英文。\n\n最后提供一个实例，以前章的 [多层感知机模型](https://tf.wiki/zh/basic/models.html#mlp) 为例展示 TensorBoard 的使用："},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"import tensorflow as tf\nfrom zh.model.mnist.mlp import MLP\nfrom zh.model.utils import MNISTLoader\n\nnum_batches = 10000\nbatch_size = 50\nlearning_rate = 0.001\nmodel = MLP()\ndata_loader = MNISTLoader()\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\nsummary_writer = tf.summary.create_file_writer('./tensorboard')     # 实例化记录器\nfor batch_index in range(num_batches):\n    X, y = data_loader.get_batch(batch_size)\n    with tf.GradientTape() as tape:\n        y_pred = model(X)\n        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n        loss = tf.reduce_mean(loss)\n        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n        with summary_writer.as_default():                           # 指定记录器\n            tf.summary.scalar(\"loss\", loss, step=batch_index)       # 将当前损失函数的值写入记录器\n    grads = tape.gradient(loss, model.variables)\n    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
{"cells":[{"cell_type":"markdown","metadata":{},"source":["# TensorFlow 模型建立与训练 \n","\n","本章介绍如何使用 TensorFlow 快速搭建动态模型。\n","\n","- 模型的构建： `tf.keras.Model` 和 `tf.keras.layers`\n","\n","- 模型的损失函数： `tf.keras.losses`\n","\n","- 模型的优化器： `tf.keras.optimizer`\n","\n","- 模型的评估： `tf.keras.metrics`/"]},{"cell_type":"markdown","metadata":{},"source":["## 模型（Model）与层（Layer）\n","\n","在 TensorFlow 中，推荐使用 Keras（ `tf.keras` ）构建模型。Keras 是一个广为流行的高级神经网络 API，简单、快速而不失灵活性，现已得到 TensorFlow 的官方内置和全面支持。\n","\n","Keras 有两个重要的概念： **模型（Model）** 和 **层（Layer）** 。层将各种计算流程和变量进行了封装（例如基本的全连接层，CNN 的卷积层、池化层等），而模型则将各种层进行组织和连接，并封装成一个整体，描述了如何将输入数据通过各种层以及运算而得到输出。在需要模型调用的时候，使用 `y_pred = model(X)` 的形式即可。Keras 在 `tf.keras.layers` 下内置了深度学习中大量常用的的预定义层，同时也允许我们自定义层。\n","\n","Keras 模型以类的形式呈现，我们可以通过继承 `tf.keras.Model` 这个 Python 类来定义自己的模型。在继承类中，我们需要重写 `__init__()` （构造函数，初始化）和 `call(input)` （模型调用）两个方法，同时也可以根据需要增加自定义的方法。"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","class MyModel(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()     # Python 2 下使用 super(MyModel, self).__init__()\n","        # 此处添加初始化代码（包含 call 方法中会用到的层），例如\n","        # layer1 = tf.keras.layers.BuiltInLayer(...)\n","        # layer2 = MyCustomLayer(...)\n","\n","    def call(self, input):\n","        # 此处添加模型调用的代码（处理输入并返回输出），例如\n","        # x = layer1(input)\n","        # output = layer2(x)\n","        return output\n","\n","    # 还可以添加自定义的方法"]},{"cell_type":"markdown","metadata":{},"source":["[![../../_images/model.png](https://tf.wiki/_images/model.png)](https://tf.wiki/_images/model.png)\n","\n","Keras 模型类定义示意图 \n","\n","继承 `tf.keras.Model` 后，我们同时可以使用父类的若干方法和属性，例如在实例化类 `model = Model()` 后，可以通过 `model.variables` 这一属性直接获得模型中的所有变量，免去我们一个个显式指定变量的麻烦。\n","\n","上一章中简单的线性模型 `y_pred = a * X + b` ，我们可以通过模型类的方式编写如下："]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\narray([[nan],\n       [nan],\n       [nan]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([nan], dtype=float32)>]\n"}],"source":["x = tf.constant([[1., 2., 3.],[4., 5., 6.]])\n","y = tf.constant([10.0, 20.0])\n","\n","class Linear(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.Dense = tf.keras.layers.Dense(\n","            units=1,\n","            activation=None,\n","            kernel_initializer=tf.zeros_initializer,\n","            bias_initializer=tf.zeros_initializer,\n","        )\n","    def call(self, input):\n","        out = self.Dense(input)\n","        return out\n","\n","# 模型训练\n","model = Linear()\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n","for i in range(100):\n","    with tf.GradientTape() as tape:\n","        y_pred = model(x)\n","        loss = tf.reduce_sum(tf.square(y_pred - y))\n","    grads = tape.gradient(loss, model.variables)\n","    optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))\n","print(model.variables)"]},{"cell_type":"markdown","metadata":{},"source":["这里，我们没有显式地声明 `a` 和 `b` 两个变量并写出 `y_pred = a * X + b` 这一线性变换，而是建立了一个继承了 `tf.keras.Model` 的模型类 `Linear` 。这个类在初始化部分实例化了一个 **全连接层** （ `tf.keras.layers.Dense` ），并在 call 方法中对这个层进行调用，实现了线性变换的计算。如果需要显式地声明自己的变量并使用变量进行自定义运算，或者希望了解 Keras 层的内部原理，请参考 [自定义层](https://tf.wiki/zh/basic/models.html#custom-layer)。\n","\n","Keras 的全连接层：线性变换 + 激活函数\n","\n","[全连接层](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) （Fully-connected Layer，`tf.keras.layers.Dense` ）是 Keras 中最基础和常用的层之一，对输入矩阵 ![A](https://tf.wiki/_images/math/211284f68205c3e66773eaf026f32a0acdd3dfb3.png) 进行 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 的线性变换 + 激活函数操作。如果不指定激活函数，即是纯粹的线性变换 ![AW + b](https://tf.wiki/_images/math/7195d0ad48bfa4fb60a47b82eb81b21d21ef9d9f.png)。具体而言，给定输入张量 `input = [batch_size, input_dim]` ，该层对输入张量首先进行 `tf.matmul(input, kernel) + bias` 的线性变换（ `kernel` 和 `bias` 是层中可训练的变量），然后对线性变换后张量的每个元素通过激活函数 `activation` ，从而输出形状为 `[batch_size, units]` 的二维张量。\n","\n","[![../../_images/dense.png](https://tf.wiki/_images/dense.png)](https://tf.wiki/_images/dense.png)\n","\n","其包含的主要参数如下：\n","\n","- `units` ：输出张量的维度；\n","- `activation` ：激活函数，对应于 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 中的 ![f](https://tf.wiki/_images/math/5b7752c757e0b691a80ab8227eadb8a8389dc58a.png) ，默认为无激活函数（ `a(x) = x` ）。常用的激活函数包括 `tf.nn.relu` 、 `tf.nn.tanh` 和 `tf.nn.sigmoid` ；\n","- `use_bias` ：是否加入偏置向量 `bias` ，即 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 中的 ![b](https://tf.wiki/_images/math/68c7c8c65602677ab56cf7fd88002023f0edc575.png)。默认为 `True` ；\n","- `kernel_initializer` 、 `bias_initializer` ：权重矩阵 `kernel` 和偏置向量 `bias` 两个变量的初始化器。默认为 `tf.glorot_uniform_initializer` [1](https://tf.wiki/zh/basic/models.html#glorot) 。设置为 `tf.zeros_initializer` 表示将两个变量均初始化为全 0；\n","\n","该层包含权重矩阵 `kernel = [input_dim, units]` 和偏置向量 `bias = [units]` [2](https://tf.wiki/zh/basic/models.html#broadcast) 两个可训练变量，对应于 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 中的 ![W](https://tf.wiki/_images/math/1fbee781f84569077719a167b64e12064360fac1.png) 和 ![b](https://tf.wiki/_images/math/68c7c8c65602677ab56cf7fd88002023f0edc575.png)。\n","\n","这里着重从数学矩阵运算和线性变换的角度描述了全连接层。基于神经元建模的描述可参考 [后文介绍](https://tf.wiki/zh/basic/models.html#neuron) 。\n","\n","- [1](https://tf.wiki/zh/basic/models.html#id3)\n","\n","  Keras 中的很多层都默认使用 `tf.glorot_uniform_initializer` 初始化变量，关于该初始化器可参考 https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer 。\n","\n","- [2](https://tf.wiki/zh/basic/models.html#id4)\n","\n","  你可能会注意到， `tf.matmul(input, kernel)` 的结果是一个形状为 `[batch_size, units]` 的二维矩阵，这个二维矩阵要如何与形状为 `[units]` 的一维偏置向量 `bias` 相加呢？事实上，这里是 TensorFlow 的 Broadcasting 机制在起作用，该加法运算相当于将二维矩阵的每一行加上了 `Bias` 。Broadcasting 机制的具体介绍可见 https://www.tensorflow.org/xla/broadcasting 。\n","\n","为什么模型类是重载 `call()` 方法而不是 `__call__()` 方法？\n","\n","在 Python 中，对类的实例 `myClass` 进行形如 `myClass()` 的调用等价于 `myClass.__call__()` （具体请见本章初 “前置知识” 的 `__call__()` 部分）。那么看起来，为了使用 `y_pred = model(X)` 的形式调用模型类，应该重写 `__call__()` 方法才对呀？原因是 Keras 在模型调用的前后还需要有一些自己的内部操作，所以暴露出一个专门用于重载的 `call()` 方法。 `tf.keras.Model` 这一父类已经包含 `__call__()` 的定义。 `__call__()` 中主要调用了 `call()` 方法，同时还需要在进行一些 keras 的内部操作。这里，我们通过继承 `tf.keras.Model` 并重载 `call()` 方法，即可在保持 keras 结构的同时加入模型调用的代码。"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## 基础示例：多层感知机（MLP）\n","\n","我们从编写一个最简单的 [多层感知机](https://zh.wikipedia.org/wiki/多层感知器) （Multilayer Perceptron, MLP），或者说 “多层全连接神经网络” 开始，介绍 TensorFlow 的模型编写方式。在这一部分，我们依次进行以下步骤：\n","\n","- 使用 `tf.keras.datasets` 获得数据集并预处理\n","- 使用 `tf.keras.Model` 和 `tf.keras.layers` 构建模型\n","- 构建模型训练流程，使用 `tf.keras.losses` 计算损失函数，并使用 `tf.keras.optimizer` 优化模型\n","- 构建模型评估流程，使用 `tf.keras.metrics` 计算评估指标\n","\n","基础知识和原理\n","\n","- UFLDL 教程 [Multi-Layer Neural Network](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/) 一节；\n","- 斯坦福课程 [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/) 中的 “Neural Networks Part 1 ~ 3” 部分。\n","\n","这里，我们使用多层感知机完成 MNIST 手写体数字图片数据集 [[LeCun1998\\]](https://tf.wiki/zh/basic/models.html#lecun1998) 的分类任务。\n","\n","![../../_images/mnist_0-9.png](https://tf.wiki/_images/mnist_0-9.png)\n","\n","MNIST 手写体数字图片示例 \n","\n","### 数据获取及预处理： `tf.keras.datasets`\n","\n","先进行预备工作，实现一个简单的 `MNISTLoader` 类来读取 MNIST 数据集数据。这里使用了 `tf.keras.datasets` 快速载入 MNIST 数据集。"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# 数据获取及预处理\n","import numpy as np\n","class MNISTLoader():\n","    def __init__(self):\n","        mnist = tf.keras.datasets.mnist\n","        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n","        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n","        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n","        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n","        self.train_label = self.train_label.astype(np.int32)    # [60000]\n","        self.test_label = self.test_label.astype(np.int32)      # [10000]\n","        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n","\n","    def get_batch(self, batch_size):\n","        # 从数据集中随机取出batch_size个元素并返回\n","        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n","        return self.train_data[index, :], self.train_label[index]"]},{"cell_type":"markdown","metadata":{},"source":["`mnist = tf.keras.datasets.mnist` 将从网络上自动下载 MNIST 数据集并加载。如果运行时出现网络连接错误，可以从 https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 或 https://s3.amazonaws.com/img-datasets/mnist.npz 下载 MNIST 数据集 `mnist.npz` 文件，并放置于用户目录的 `.keras/dataset` 目录下（Windows 下用户目录为 `C:\\Users\\用户名` ，Linux 下用户目录为 `/home/用户名` ）。\n","\n","TensorFlow 的图像数据表示\n","\n","在 TensorFlow 中，图像数据集的一种典型表示是 `[图像数目，长，宽，色彩通道数]` 的四维张量。在上面的 `DataLoader` 类中， `self.train_data` 和 `self.test_data` 分别载入了 60,000 和 10,000 张大小为 `28*28` 的手写体数字图片。由于这里读入的是灰度图片，色彩通道数为 1（彩色 RGB 图像色彩通道数为 3），所以我们使用 `np.expand_dims()` 函数为图像数据手动在最后添加一维通道。"]},{"cell_type":"markdown","metadata":{},"source":["### 模型的构建： `tf.keras.Model` 和 `tf.keras.layers`\n","\n","多层感知机的模型类实现与上面的线性模型类似，使用 `tf.keras.Model` 和 `tf.keras.layers` 构建，所不同的地方在于层数增加了（顾名思义，“多层” 感知机），以及引入了非线性激活函数（这里使用了 [ReLU 函数](https://zh.wikipedia.org/wiki/线性整流函数) ， 即下方的 `activation=tf.nn.relu` ）。该模型输入一个向量（比如这里是拉直的 `1×784` 手写体数字图片），输出 10 维的向量，分别代表这张图片属于 0 到 9 的概率。"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["class MLP(tf.keras.Model):\n","    def __init__(self):\n","        super().__init__()\n","        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n","        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n","        self.dense2 = tf.keras.layers.Dense(units=10)\n","\n","    def call(self, inputs):         # [batch_size, 28, 28, 1]\n","        x = self.flatten(inputs)    # [batch_size, 784]\n","        x = self.dense1(x)          # [batch_size, 100]\n","        x = self.dense2(x)          # [batch_size, 10]\n","        output = tf.nn.softmax(x)\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["softmax 函数\n","\n","这里，因为我们希望输出 “输入图片分别属于 0 到 9 的概率”，也就是一个 10 维的离散概率分布，所以我们希望这个 10 维向量至少满足两个条件：\n","\n","- 该向量中的每个元素均在 ![[0, 1]](https://tf.wiki/_images/math/8027137b3073a7f5ca4e45ba2d030dcff154eca4.png) 之间；\n","- 该向量的所有元素之和为 1。\n","\n","为了使得模型的输出能始终满足这两个条件，我们使用 [Softmax 函数](https://zh.wikipedia.org/wiki/Softmax函数) （归一化指数函数， `tf.nn.softmax` ）对模型的原始输出进行归一化。其形式为 ![\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}](https://tf.wiki/_images/math/7d714874b555007ada90b5315e5fa2ffa0e5e2ee.png) 。不仅如此，softmax 函数能够凸显原始向量中最大的值，并抑制远低于最大值的其他分量，这也是该函数被称作 softmax 函数的原因（即平滑化的 argmax 函数）。\n","\n","[![../../_images/mlp.png](https://tf.wiki/_images/mlp.png)](https://tf.wiki/_images/mlp.png)\n","\n","MLP 模型示意图 \n"]},{"cell_type":"markdown","metadata":{},"source":["### 模型的训练： `tf.keras.losses` 和 `tf.keras.optimizer`\n","\n","定义一些模型超参数："]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["num_epochs = 5\n","batch_size = 50\n","learning_rate = 0.001"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["model = MLP()\n","data_loader = MNISTLoader()\n","optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["然后迭代进行以下步骤：\n","\n","- 从 DataLoader 中随机取一批训练数据；\n","- 将这批数据送入模型，计算出模型的预测值；\n","- 将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 `tf.keras.losses` 中的交叉熵函数作为损失函数；\n","- 计算损失函数关于模型变量的导数；\n","- 将求出的导数值传入优化器，使用优化器的 `apply_gradients` 方法更新模型参数以最小化损失函数（优化器的详细使用方法见 [前章](https://tf.wiki/en/basic/basic.html#optimizer) ）。\n","\n","具体代码实现如下："]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"11765\nbatch 5231: loss 0.020297\nbatch 5232: loss 0.009016\nbatch 5233: loss 0.029058\nbatch 5234: loss 0.020301\nbatch 5235: loss 0.065630\nbatch 5236: loss 0.117782\nbatch 5237: loss 0.023475\nbatch 5238: loss 0.017457\nbatch 5239: loss 0.031909\nbatch 5240: loss 0.090876\nbatch 5241: loss 0.029181\nbatch 5242: loss 0.052658\nbatch 5243: loss 0.045131\nbatch 5244: loss 0.020806\nbatch 5245: loss 0.072734\nbatch 5246: loss 0.035175\nbatch 5247: loss 0.021386\nbatch 5248: loss 0.018279\nbatch 5249: loss 0.013514\nbatch 5250: loss 0.028197\nbatch 5251: loss 0.020028\nbatch 5252: loss 0.011727\nbatch 5253: loss 0.129560\nbatch 5254: loss 0.027805\nbatch 5255: loss 0.008097\nbatch 5256: loss 0.141144\nbatch 5257: loss 0.015935\nbatch 5258: loss 0.031550\nbatch 5259: loss 0.041442\nbatch 5260: loss 0.025335\nbatch 5261: loss 0.011035\nbatch 5262: loss 0.088995\nbatch 5263: loss 0.021100\nbatch 5264: loss 0.147815\nbatch 5265: loss 0.017561\nbatch 5266: loss 0.019981\nbatch 5267: loss 0.005434\nbatch 5268: loss 0.064148\nbatch 5269: loss 0.012365\nbatch 5270: loss 0.078926\nbatch 5271: loss 0.040052\nbatch 5272: loss 0.085506\nbatch 5273: loss 0.026090\nbatch 5274: loss 0.007272\nbatch 5275: loss 0.014189\nbatch 5276: loss 0.046131\nbatch 5277: loss 0.083768\nbatch 5278: loss 0.017818\nbatch 5279: loss 0.091558\nbatch 5280: loss 0.096956\nbatch 5281: loss 0.101973\nbatch 5282: loss 0.034004\nbatch 5283: loss 0.048155\nbatch 5284: loss 0.022279\nbatch 5285: loss 0.078414\nbatch 5286: loss 0.034572\nbatch 5287: loss 0.029450\nbatch 5288: loss 0.025670\nbatch 5289: loss 0.015103\nbatch 5290: loss 0.024697\nbatch 5291: loss 0.057645\nbatch 5292: loss 0.049676\nbatch 5293: loss 0.016376\nbatch 5294: loss 0.044099\nbatch 5295: loss 0.043420\nbatch 5296: loss 0.045947\nbatch 5297: loss 0.061365\nbatch 5298: loss 0.056653\nbatch 5299: loss 0.055882\nbatch 5300: loss 0.192469\nbatch 5301: loss 0.017658\nbatch 5302: loss 0.042175\nbatch 5303: loss 0.029990\nbatch 5304: loss 0.055777\nbatch 5305: loss 0.050597\nbatch 5306: loss 0.123367\nbatch 5307: loss 0.061678\nbatch 5308: loss 0.005600\nbatch 5309: loss 0.029705\nbatch 5310: loss 0.052337\nbatch 5311: loss 0.033153\nbatch 5312: loss 0.024984\nbatch 5313: loss 0.012423\nbatch 5314: loss 0.027031\nbatch 5315: loss 0.008529\nbatch 5316: loss 0.037755\nbatch 5317: loss 0.065220\nbatch 5318: loss 0.035361\nbatch 5319: loss 0.090383\nbatch 5320: loss 0.063525\nbatch 5321: loss 0.061625\nbatch 5322: loss 0.035492\nbatch 5323: loss 0.014524\nbatch 5324: loss 0.017552\nbatch 5325: loss 0.094784\nbatch 5326: loss 0.043062\nbatch 5327: loss 0.024303\nbatch 5328: loss 0.009594\nbatch 5329: loss 0.059445\nbatch 5330: loss 0.287413\nbatch 5331: loss 0.009006\nbatch 5332: loss 0.006298\nbatch 5333: loss 0.099910\nbatch 5334: loss 0.090632\nbatch 5335: loss 0.056686\nbatch 5336: loss 0.037473\nbatch 5337: loss 0.057110\nbatch 5338: loss 0.020107\nbatch 5339: loss 0.032791\nbatch 5340: loss 0.038268\nbatch 5341: loss 0.175458\nbatch 5342: loss 0.017830\nbatch 5343: loss 0.065809\nbatch 5344: loss 0.006853\nbatch 5345: loss 0.016179\nbatch 5346: loss 0.059512\nbatch 5347: loss 0.231154\nbatch 5348: loss 0.035685\nbatch 5349: loss 0.037402\nbatch 5350: loss 0.054074\nbatch 5351: loss 0.072829\nbatch 5352: loss 0.015553\nbatch 5353: loss 0.021255\nbatch 5354: loss 0.035838\nbatch 5355: loss 0.020703\nbatch 5356: loss 0.041821\nbatch 5357: loss 0.011181\nbatch 5358: loss 0.015231\nbatch 5359: loss 0.045506\nbatch 5360: loss 0.038861\nbatch 5361: loss 0.015529\nbatch 5362: loss 0.068731\nbatch 5363: loss 0.031487\nbatch 5364: loss 0.109654\nbatch 5365: loss 0.154970\nbatch 5366: loss 0.173881\nbatch 5367: loss 0.108482\nbatch 5368: loss 0.008923\nbatch 5369: loss 0.021625\nbatch 5370: loss 0.054415\nbatch 5371: loss 0.028021\nbatch 5372: loss 0.102421\nbatch 5373: loss 0.048583\nbatch 5374: loss 0.030578\nbatch 5375: loss 0.014834\nbatch 5376: loss 0.040707\nbatch 5377: loss 0.065220\nbatch 5378: loss 0.013229\nbatch 5379: loss 0.031000\nbatch 5380: loss 0.066527\nbatch 5381: loss 0.042315\nbatch 5382: loss 0.136291\nbatch 5383: loss 0.039680\nbatch 5384: loss 0.039025\nbatch 5385: loss 0.015790\nbatch 5386: loss 0.056742\nbatch 5387: loss 0.013194\nbatch 5388: loss 0.030067\nbatch 5389: loss 0.053025\nbatch 5390: loss 0.059207\nbatch 5391: loss 0.040818\nbatch 5392: loss 0.019486\nbatch 5393: loss 0.058420\nbatch 5394: loss 0.029836\nbatch 5395: loss 0.010135\nbatch 5396: loss 0.055515\nbatch 5397: loss 0.026586\nbatch 5398: loss 0.014771\nbatch 5399: loss 0.043084\nbatch 5400: loss 0.088884\nbatch 5401: loss 0.049932\nbatch 5402: loss 0.073866\nbatch 5403: loss 0.026314\nbatch 5404: loss 0.012560\nbatch 5405: loss 0.030761\nbatch 5406: loss 0.076775\nbatch 5407: loss 0.014775\nbatch 5408: loss 0.077604\nbatch 5409: loss 0.033764\nbatch 5410: loss 0.106117\nbatch 5411: loss 0.015050\nbatch 5412: loss 0.048727\nbatch 5413: loss 0.047595\nbatch 5414: loss 0.060300\nbatch 5415: loss 0.019637\nbatch 5416: loss 0.114680\nbatch 5417: loss 0.007198\nbatch 5418: loss 0.074592\nbatch 5419: loss 0.034253\nbatch 5420: loss 0.009959\nbatch 5421: loss 0.145483\nbatch 5422: loss 0.067076\nbatch 5423: loss 0.034811\nbatch 5424: loss 0.047538\nbatch 5425: loss 0.023253\nbatch 5426: loss 0.025561\nbatch 5427: loss 0.056400\nbatch 5428: loss 0.043943\nbatch 5429: loss 0.048398\nbatch 5430: loss 0.017905\nbatch 5431: loss 0.046085\nbatch 5432: loss 0.011066\nbatch 5433: loss 0.174442\nbatch 5434: loss 0.110007\nbatch 5435: loss 0.017795\nbatch 5436: loss 0.013770\nbatch 5437: loss 0.014938\nbatch 5438: loss 0.009225\nbatch 5439: loss 0.079042\nbatch 5440: loss 0.029976\nbatch 5441: loss 0.043585\nbatch 5442: loss 0.065459\nbatch 5443: loss 0.010163\nbatch 5444: loss 0.008753\nbatch 5445: loss 0.044039\nbatch 5446: loss 0.158596\nbatch 5447: loss 0.076248\nbatch 5448: loss 0.056104\nbatch 5449: loss 0.025018\nbatch 5450: loss 0.153599\nbatch 5451: loss 0.092945\nbatch 5452: loss 0.195323\nbatch 5453: loss 0.077505\nbatch 5454: loss 0.027739\nbatch 5455: loss 0.015808\nbatch 5456: loss 0.055565\nbatch 5457: loss 0.023542\nbatch 5458: loss 0.054958\nbatch 5459: loss 0.045801\nbatch 5460: loss 0.069098\nbatch 5461: loss 0.107352\nbatch 5462: loss 0.070008\nbatch 5463: loss 0.098997\nbatch 5464: loss 0.114398\nbatch 5465: loss 0.006393\nbatch 5466: loss 0.158296\nbatch 5467: loss 0.079113\nbatch 5468: loss 0.175164\nbatch 5469: loss 0.151436\nbatch 5470: loss 0.021923\nbatch 5471: loss 0.134450\nbatch 5472: loss 0.065121\nbatch 5473: loss 0.022807\nbatch 5474: loss 0.018255\nbatch 5475: loss 0.028625\nbatch 5476: loss 0.107481\nbatch 5477: loss 0.037284\nbatch 5478: loss 0.091000\nbatch 5479: loss 0.071441\nbatch 5480: loss 0.026130\nbatch 5481: loss 0.067784\nbatch 5482: loss 0.050262\nbatch 5483: loss 0.028971\nbatch 5484: loss 0.010834\nbatch 5485: loss 0.014248\nbatch 5486: loss 0.211344\nbatch 5487: loss 0.054587\nbatch 5488: loss 0.082004\nbatch 5489: loss 0.028667\nbatch 5490: loss 0.130235\nbatch 5491: loss 0.016728\nbatch 5492: loss 0.075689\nbatch 5493: loss 0.067427\nbatch 5494: loss 0.076684\nbatch 5495: loss 0.090568\nbatch 5496: loss 0.042984\nbatch 5497: loss 0.014934\nbatch 5498: loss 0.085053\nbatch 5499: loss 0.046598\nbatch 5500: loss 0.131613\nbatch 5501: loss 0.014734\nbatch 5502: loss 0.014640\nbatch 5503: loss 0.009954\nbatch 5504: loss 0.072808\nbatch 5505: loss 0.114520\nbatch 5506: loss 0.045285\nbatch 5507: loss 0.025799\nbatch 5508: loss 0.015842\nbatch 5509: loss 0.029903\nbatch 5510: loss 0.017692\nbatch 5511: loss 0.029905\nbatch 5512: loss 0.017319\nbatch 5513: loss 0.025811\nbatch 5514: loss 0.013690\nbatch 5515: loss 0.058182\nbatch 5516: loss 0.027532\nbatch 5517: loss 0.021702\nbatch 5518: loss 0.006457\nbatch 5519: loss 0.085447\nbatch 5520: loss 0.029572\nbatch 5521: loss 0.011266\nbatch 5522: loss 0.024264\nbatch 5523: loss 0.055457\nbatch 5524: loss 0.027873\nbatch 5525: loss 0.009158\nbatch 5526: loss 0.016704\nbatch 5527: loss 0.106466\nbatch 5528: loss 0.007828\nbatch 5529: loss 0.033441\nbatch 5530: loss 0.011337\nbatch 5531: loss 0.094911\nbatch 5532: loss 0.037980\nbatch 5533: loss 0.081582\nbatch 5534: loss 0.048209\nbatch 5535: loss 0.089782\nbatch 5536: loss 0.020413\nbatch 5537: loss 0.126025\nbatch 5538: loss 0.065910\nbatch 5539: loss 0.046916\nbatch 5540: loss 0.009533\nbatch 5541: loss 0.039175\nbatch 5542: loss 0.087081\nbatch 5543: loss 0.043169\nbatch 5544: loss 0.014769\nbatch 5545: loss 0.058551\nbatch 5546: loss 0.024184\nbatch 5547: loss 0.062091\nbatch 5548: loss 0.093732\nbatch 5549: loss 0.003304\nbatch 5550: loss 0.074013\nbatch 5551: loss 0.092128\nbatch 5552: loss 0.046571\nbatch 5553: loss 0.045318\nbatch 5554: loss 0.028173\nbatch 5555: loss 0.020455\nbatch 5556: loss 0.026930\nbatch 5557: loss 0.044641\nbatch 5558: loss 0.064697\nbatch 5559: loss 0.090261\nbatch 5560: loss 0.023353\nbatch 5561: loss 0.090840\nbatch 5562: loss 0.092593\nbatch 5563: loss 0.030171\nbatch 5564: loss 0.039471\nbatch 5565: loss 0.009903\nbatch 5566: loss 0.079547\nbatch 5567: loss 0.031212\nbatch 5568: loss 0.024397\nbatch 5569: loss 0.181832\nbatch 5570: loss 0.055732\nbatch 5571: loss 0.006443\nbatch 5572: loss 0.122129\nbatch 5573: loss 0.189753\nbatch 5574: loss 0.049675\nbatch 5575: loss 0.065478\nbatch 5576: loss 0.031635\nbatch 5577: loss 0.060348\nbatch 5578: loss 0.007569\nbatch 5579: loss 0.016646\nbatch 5580: loss 0.011973\nbatch 5581: loss 0.223586\nbatch 5582: loss 0.060907\nbatch 5583: loss 0.012759\nbatch 5584: loss 0.057416\nbatch 5585: loss 0.047178\nbatch 5586: loss 0.039147\nbatch 5587: loss 0.007017\nbatch 5588: loss 0.008703\nbatch 5589: loss 0.030367\nbatch 5590: loss 0.030839\nbatch 5591: loss 0.026097\nbatch 5592: loss 0.075044\nbatch 5593: loss 0.066420\nbatch 5594: loss 0.039177\nbatch 5595: loss 0.071262\nbatch 5596: loss 0.037742\nbatch 5597: loss 0.044810\nbatch 5598: loss 0.016914\nbatch 5599: loss 0.046628\nbatch 5600: loss 0.065609\nbatch 5601: loss 0.011034\nbatch 5602: loss 0.056524\nbatch 5603: loss 0.119610\nbatch 5604: loss 0.037666\nbatch 5605: loss 0.091898\nbatch 5606: loss 0.076607\nbatch 5607: loss 0.031683\nbatch 5608: loss 0.106147\nbatch 5609: loss 0.242058\nbatch 5610: loss 0.122867\nbatch 5611: loss 0.029427\nbatch 5612: loss 0.116102\nbatch 5613: loss 0.028722\nbatch 5614: loss 0.050089\nbatch 5615: loss 0.050826\nbatch 5616: loss 0.020451\nbatch 5617: loss 0.028608\nbatch 5618: loss 0.090233\nbatch 5619: loss 0.025126\nbatch 5620: loss 0.070996\nbatch 5621: loss 0.133994\nbatch 5622: loss 0.009072\nbatch 5623: loss 0.128079\nbatch 5624: loss 0.034183\nbatch 5625: loss 0.038955\nbatch 5626: loss 0.043700\nbatch 5627: loss 0.013601\nbatch 5628: loss 0.081949\nbatch 5629: loss 0.027304\nbatch 5630: loss 0.051056\nbatch 5631: loss 0.054696\nbatch 5632: loss 0.103631\nbatch 5633: loss 0.054364\nbatch 5634: loss 0.056806\nbatch 5635: loss 0.089119\nbatch 5636: loss 0.084143\nbatch 5637: loss 0.114961\nbatch 5638: loss 0.096086\nbatch 5639: loss 0.031260\nbatch 5640: loss 0.067814\nbatch 5641: loss 0.036559\nbatch 5642: loss 0.050108\nbatch 5643: loss 0.013634\nbatch 5644: loss 0.006507\nbatch 5645: loss 0.008871\nbatch 5646: loss 0.057909\nbatch 5647: loss 0.053087\nbatch 5648: loss 0.059037\nbatch 5649: loss 0.022511\nbatch 5650: loss 0.118588\nbatch 5651: loss 0.011995\nbatch 5652: loss 0.038102\nbatch 5653: loss 0.019913\nbatch 5654: loss 0.022149\nbatch 5655: loss 0.102881\nbatch 5656: loss 0.006991\nbatch 5657: loss 0.082620\nbatch 5658: loss 0.158449\nbatch 5659: loss 0.066083\nbatch 5660: loss 0.024399\nbatch 5661: loss 0.055123\nbatch 5662: loss 0.020298\nbatch 5663: loss 0.009079\nbatch 5664: loss 0.066818\nbatch 5665: loss 0.009591\nbatch 5666: loss 0.086133\nbatch 5667: loss 0.033118\nbatch 5668: loss 0.086657\nbatch 5669: loss 0.010107\nbatch 5670: loss 0.009999\nbatch 5671: loss 0.018790\nbatch 5672: loss 0.032313\nbatch 5673: loss 0.074190\nbatch 5674: loss 0.099549\nbatch 5675: loss 0.108032\nbatch 5676: loss 0.023050\nbatch 5677: loss 0.008910\nbatch 5678: loss 0.038707\nbatch 5679: loss 0.045919\nbatch 5680: loss 0.027192\nbatch 5681: loss 0.053340\nbatch 5682: loss 0.028430\nbatch 5683: loss 0.022793\nbatch 5684: loss 0.085279\nbatch 5685: loss 0.107670\nbatch 5686: loss 0.031776\nbatch 5687: loss 0.078703\nbatch 5688: loss 0.015967\nbatch 5689: loss 0.071607\nbatch 5690: loss 0.071413\nbatch 5691: loss 0.035258\nbatch 5692: loss 0.043667\nbatch 5693: loss 0.060589\nbatch 5694: loss 0.227069\nbatch 5695: loss 0.030684\nbatch 5696: loss 0.042741\nbatch 5697: loss 0.025024\nbatch 5698: loss 0.033103\nbatch 5699: loss 0.165801\nbatch 5700: loss 0.054146\nbatch 5701: loss 0.030950\nbatch 5702: loss 0.032074\nbatch 5703: loss 0.032202\nbatch 5704: loss 0.125965\nbatch 5705: loss 0.040807\nbatch 5706: loss 0.063237\nbatch 5707: loss 0.044763\nbatch 5708: loss 0.017842\nbatch 5709: loss 0.035297\nbatch 5710: loss 0.090987\nbatch 5711: loss 0.017732\nbatch 5712: loss 0.009804\nbatch 5713: loss 0.015070\nbatch 5714: loss 0.010035\nbatch 5715: loss 0.019176\nbatch 5716: loss 0.172039\nbatch 5717: loss 0.101411\nbatch 5718: loss 0.023627\nbatch 5719: loss 0.033575\nbatch 5720: loss 0.112787\nbatch 5721: loss 0.082845\nbatch 5722: loss 0.022092\nbatch 5723: loss 0.050991\nbatch 5724: loss 0.035454\nbatch 5725: loss 0.019690\nbatch 5726: loss 0.005848\nbatch 5727: loss 0.012196\nbatch 5728: loss 0.056052\nbatch 5729: loss 0.165533\nbatch 5730: loss 0.013742\nbatch 5731: loss 0.027908\nbatch 5732: loss 0.039011\nbatch 5733: loss 0.052366\nbatch 5734: loss 0.012422\nbatch 5735: loss 0.095999\nbatch 5736: loss 0.033194\nbatch 5737: loss 0.086800\nbatch 5738: loss 0.101279\nbatch 5739: loss 0.031502\nbatch 5740: loss 0.054403\nbatch 5741: loss 0.044321\nbatch 5742: loss 0.028615\nbatch 5743: loss 0.032571\nbatch 5744: loss 0.010500\nbatch 5745: loss 0.019249\nbatch 5746: loss 0.001998\nbatch 5747: loss 0.069774\nbatch 5748: loss 0.005431\nbatch 5749: loss 0.021423\nbatch 5750: loss 0.007329\nbatch 5751: loss 0.115897\nbatch 5752: loss 0.006178\nbatch 5753: loss 0.083566\nbatch 5754: loss 0.037604\nbatch 5755: loss 0.069096\nbatch 5756: loss 0.026068\nbatch 5757: loss 0.079635\nbatch 5758: loss 0.096444\nbatch 5759: loss 0.095464\nbatch 5760: loss 0.092870\nbatch 5761: loss 0.080622\nbatch 5762: loss 0.044978\nbatch 5763: loss 0.017010\nbatch 5764: loss 0.018702\nbatch 5765: loss 0.054972\nbatch 5766: loss 0.056055\nbatch 5767: loss 0.051525\nbatch 5768: loss 0.023554\nbatch 5769: loss 0.036640\nbatch 5770: loss 0.009570\nbatch 5771: loss 0.058102\nbatch 5772: loss 0.060884\nbatch 5773: loss 0.031692\nbatch 5774: loss 0.015916\nbatch 5775: loss 0.176987\nbatch 5776: loss 0.061232\nbatch 5777: loss 0.112635\nbatch 5778: loss 0.090544\nbatch 5779: loss 0.028846\nbatch 5780: loss 0.021930\nbatch 5781: loss 0.277700\nbatch 5782: loss 0.085119\nbatch 5783: loss 0.020571\nbatch 5784: loss 0.170443\nbatch 5785: loss 0.066269\nbatch 5786: loss 0.026243\nbatch 5787: loss 0.008787\nbatch 5788: loss 0.009993\nbatch 5789: loss 0.092129\nbatch 5790: loss 0.020328\nbatch 5791: loss 0.138322\nbatch 5792: loss 0.139796\nbatch 5793: loss 0.235575\nbatch 5794: loss 0.036370\nbatch 5795: loss 0.039612\nbatch 5796: loss 0.063467\nbatch 5797: loss 0.043494\nbatch 5798: loss 0.032144\nbatch 5799: loss 0.004723\nbatch 5800: loss 0.035199\nbatch 5801: loss 0.006562\nbatch 5802: loss 0.021886\nbatch 5803: loss 0.107623\nbatch 5804: loss 0.022294\nbatch 5805: loss 0.029147\nbatch 5806: loss 0.029953\nbatch 5807: loss 0.081476\nbatch 5808: loss 0.063209\nbatch 5809: loss 0.014410\nbatch 5810: loss 0.045461\nbatch 5811: loss 0.066354\nbatch 5812: loss 0.146202\nbatch 5813: loss 0.037052\nbatch 5814: loss 0.012172\nbatch 5815: loss 0.051751\nbatch 5816: loss 0.180853\nbatch 5817: loss 0.030264\nbatch 5818: loss 0.061694\nbatch 5819: loss 0.029863\nbatch 5820: loss 0.020067\nbatch 5821: loss 0.060415\nbatch 5822: loss 0.019274\nbatch 5823: loss 0.008965\nbatch 5824: loss 0.023699\nbatch 5825: loss 0.022431\nbatch 5826: loss 0.059381\nbatch 5827: loss 0.020001\nbatch 5828: loss 0.046476\nbatch 5829: loss 0.047404\nbatch 5830: loss 0.052352\nbatch 5831: loss 0.026692\nbatch 5832: loss 0.079084\nbatch 5833: loss 0.042625\nbatch 5834: loss 0.046992\nbatch 5835: loss 0.015583\nbatch 5836: loss 0.172875\nbatch 5837: loss 0.049473\nbatch 5838: loss 0.116237\nbatch 5839: loss 0.008270\nbatch 5840: loss 0.053666\nbatch 5841: loss 0.070285\nbatch 5842: loss 0.079942\nbatch 5843: loss 0.021635\nbatch 5844: loss 0.099709\nbatch 5845: loss 0.011211\nbatch 5846: loss 0.080147\nbatch 5847: loss 0.036421\nbatch 5848: loss 0.025597\nbatch 5849: loss 0.044826\nbatch 5850: loss 0.038561\nbatch 5851: loss 0.064198\nbatch 5852: loss 0.052562\nbatch 5853: loss 0.008878\nbatch 5854: loss 0.018844\nbatch 5855: loss 0.053173\nbatch 5856: loss 0.041503\nbatch 5857: loss 0.005159\nbatch 5858: loss 0.053634\nbatch 5859: loss 0.024289\nbatch 5860: loss 0.031325\nbatch 5861: loss 0.023589\nbatch 5862: loss 0.220557\nbatch 5863: loss 0.076714\nbatch 5864: loss 0.031794\nbatch 5865: loss 0.023256\nbatch 5866: loss 0.083791\nbatch 5867: loss 0.101643\nbatch 5868: loss 0.032051\nbatch 5869: loss 0.006905\nbatch 5870: loss 0.067725\nbatch 5871: loss 0.021443\nbatch 5872: loss 0.020823\nbatch 5873: loss 0.095495\nbatch 5874: loss 0.033024\nbatch 5875: loss 0.126533\nbatch 5876: loss 0.008127\nbatch 5877: loss 0.011407\nbatch 5878: loss 0.095876\nbatch 5879: loss 0.023202\nbatch 5880: loss 0.032027\nbatch 5881: loss 0.019735\nbatch 5882: loss 0.034969\nbatch 5883: loss 0.090282\nbatch 5884: loss 0.029198\nbatch 5885: loss 0.096754\nbatch 5886: loss 0.015281\nbatch 5887: loss 0.042325\nbatch 5888: loss 0.016756\nbatch 5889: loss 0.055072\nbatch 5890: loss 0.079240\nbatch 5891: loss 0.248210\nbatch 5892: loss 0.020258\nbatch 5893: loss 0.011171\nbatch 5894: loss 0.042657\nbatch 5895: loss 0.025183\nbatch 5896: loss 0.064538\nbatch 5897: loss 0.035846\nbatch 5898: loss 0.178150\nbatch 5899: loss 0.005632\nbatch 5900: loss 0.035896\nbatch 5901: loss 0.025186\nbatch 5902: loss 0.061100\nbatch 5903: loss 0.051221\nbatch 5904: loss 0.050243\nbatch 5905: loss 0.008612\nbatch 5906: loss 0.011646\nbatch 5907: loss 0.100017\nbatch 5908: loss 0.125447\nbatch 5909: loss 0.087248\nbatch 5910: loss 0.036854\nbatch 5911: loss 0.070032\nbatch 5912: loss 0.084455\nbatch 5913: loss 0.077805\nbatch 5914: loss 0.032322\nbatch 5915: loss 0.019242\nbatch 5916: loss 0.123306\nbatch 5917: loss 0.013964\nbatch 5918: loss 0.011344\nbatch 5919: loss 0.051671\nbatch 5920: loss 0.008036\nbatch 5921: loss 0.064640\nbatch 5922: loss 0.042883\nbatch 5923: loss 0.022924\nbatch 5924: loss 0.012001\nbatch 5925: loss 0.060813\nbatch 5926: loss 0.100166\nbatch 5927: loss 0.092869\nbatch 5928: loss 0.053782\nbatch 5929: loss 0.021992\nbatch 5930: loss 0.089691\nbatch 5931: loss 0.034704\nbatch 5932: loss 0.010348\nbatch 5933: loss 0.009240\nbatch 5934: loss 0.080263\nbatch 5935: loss 0.019689\nbatch 5936: loss 0.063733\nbatch 5937: loss 0.122778\nbatch 5938: loss 0.012258\nbatch 5939: loss 0.100472\nbatch 5940: loss 0.046109\nbatch 5941: loss 0.027309\nbatch 5942: loss 0.061962\nbatch 5943: loss 0.057996\nbatch 5944: loss 0.114052\nbatch 5945: loss 0.046104\nbatch 5946: loss 0.029120\nbatch 5947: loss 0.059654\nbatch 5948: loss 0.048634\nbatch 5949: loss 0.026655\nbatch 5950: loss 0.024300\nbatch 5951: loss 0.061434\nbatch 5952: loss 0.067828\nbatch 5953: loss 0.173068\nbatch 5954: loss 0.150854\nbatch 5955: loss 0.049302\nbatch 5956: loss 0.048101\nbatch 5957: loss 0.035896\nbatch 5958: loss 0.044683\nbatch 5959: loss 0.063191\nbatch 5960: loss 0.078154\nbatch 5961: loss 0.020749\nbatch 5962: loss 0.019105\nbatch 5963: loss 0.046581\nbatch 5964: loss 0.010610\nbatch 5965: loss 0.021095\nbatch 5966: loss 0.023881\nbatch 5967: loss 0.007530\nbatch 5968: loss 0.053921\nbatch 5969: loss 0.012975\nbatch 5970: loss 0.010116\nbatch 5971: loss 0.012072\nbatch 5972: loss 0.054830\nbatch 5973: loss 0.063219\nbatch 5974: loss 0.016752\nbatch 5975: loss 0.007266\nbatch 5976: loss 0.074455\nbatch 5977: loss 0.081908\nbatch 5978: loss 0.045541\nbatch 5979: loss 0.015453\nbatch 5980: loss 0.040620\nbatch 5981: loss 0.020065\nbatch 5982: loss 0.028701\nbatch 5983: loss 0.018105\nbatch 5984: loss 0.019955\nbatch 5985: loss 0.084425\nbatch 5986: loss 0.037267\nbatch 5987: loss 0.023941\nbatch 5988: loss 0.104235\nbatch 5989: loss 0.008229\nbatch 5990: loss 0.050056\nbatch 5991: loss 0.036505\nbatch 5992: loss 0.065608\nbatch 5993: loss 0.079467\nbatch 5994: loss 0.049052\nbatch 5995: loss 0.071970\nbatch 5996: loss 0.041711\nbatch 5997: loss 0.061054\nbatch 5998: loss 0.039787\nbatch 5999: loss 0.093269\n"}],"source":["num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n","for batch_index in range(num_batches):\n","    X, y = data_loader.get_batch(batch_size)\n","    with tf.GradientTape() as tape:\n","        y_pred = model(X)\n","        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n","        loss = tf.reduce_mean(loss)\n","        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n","    grads = tape.gradient(loss, model.variables)\n","    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"]},{"cell_type":"markdown","metadata":{},"source":["交叉熵（cross entropy）与 `tf.keras.losses`\n","\n","你或许注意到了，在这里，我们没有显式地写出一个损失函数，而是使用了 `tf.keras.losses` 中的 `sparse_categorical_crossentropy` （交叉熵）函数，将模型的预测值 `y_pred` 与真实的标签值 `y` 作为函数参数传入，由 Keras 帮助我们计算损失函数的值。\n","\n","交叉熵作为损失函数，在分类问题中被广泛应用。其离散形式为 ![H(y, \\hat{y}) = -\\sum_{i=1}^{n}y_i \\log(\\hat{y_i})](https://tf.wiki/_images/math/63ad0688c80b4c83b2a6e0a542b741ed8f9ff79f.png) ，其中 ![y](https://tf.wiki/_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png) 为真实概率分布， ![\\hat{y}](https://tf.wiki/_images/math/1257829df10bd602e03553570cadfe2328fd1d91.png) 为预测概率分布， ![n](https://tf.wiki/_images/math/5a939c5280da7202ca4531f175a7780ad5e1f80a.png) 为分类任务的类别个数。预测概率分布与真实分布越接近，则交叉熵的值越小，反之则越大。更具体的介绍及其在机器学习中的应用可参考 [这篇博客文章](https://blog.csdn.net/tsyccnh/article/details/79163834) 。\n","\n","在 `tf.keras` 中，有两个交叉熵相关的损失函数 `tf.keras.losses.categorical_crossentropy` 和 `tf.keras.losses.sparse_categorical_crossentropy` 。其中 sparse 的含义是，真实的标签值 `y_true` 可以直接传入 int 类型的标签类别。具体而言：\n","\n","```\n","loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n","```\n","\n","与\n","\n","```\n","loss = tf.keras.losses.categorical_crossentropy(\n","    y_true=tf.one_hot(y, depth=tf.shape(y_pred)[-1]),\n","    y_pred=y_pred\n",")\n","```"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"test accuracy: 0.978000\n"}],"source":["sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","num_batches = int(data_loader.num_test_data // batch_size)\n","for batch_index in range(num_batches):\n","    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n","    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n","    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n","print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}
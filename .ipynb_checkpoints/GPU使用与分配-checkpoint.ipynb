{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `tf.config`：GPU 的使用与分配 *\n",
    "\n",
    "### 指定当前程序使用的 GPU\n",
    "\n",
    "很多时候的场景是：实验室 / 公司研究组里有许多学生 / 研究员需要共同使用一台多 GPU 的工作站，而默认情况下 TensorFlow 会使用其所能够使用的所有 GPU，这时就需要合理分配显卡资源。\n",
    "\n",
    "首先，通过 `tf.config.experimental.list_physical_devices` ，我们可以获得当前主机上某种特定运算设备类型（如 `GPU` 或 `CPU` ）的列表，例如，在一台具有 4 块 GPU 和一个 CPU 的工作站上运行以下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-26T02:32:33.633689Z",
     "start_time": "2019-10-26T02:32:31.015700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "cpus = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "print(gpus, cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出：\n",
    "\n",
    "```\n",
    "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
    " PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU'),\n",
    " PhysicalDevice(name='/physical_device:GPU:2', device_type='GPU'),\n",
    " PhysicalDevice(name='/physical_device:GPU:3', device_type='GPU')]\n",
    "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
    "```\n",
    "\n",
    "可见，该工作站具有 4 块 GPU：`GPU:0` 、 `GPU:1` 、 `GPU:2` 、 `GPU:3` ，以及一个 CPU `CPU:0` 。\n",
    "\n",
    "然后，通过 `tf.config.experimental.set_visible_devices` ，可以设置当前程序可见的设备范围（当前程序只会使用自己可见的设备，不可见的设备不会被当前程序使用）。例如，如果在上述 4 卡的机器中我们需要限定当前程序只使用下标为 0、1 的两块显卡（`GPU:0` 和 `GPU:1`），可以使用以下代码："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[0:2], device_type='GPU')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小技巧\n",
    "\n",
    "使用环境变量 `CUDA_VISIBLE_DEVICES` 也可以控制程序所使用的 GPU。假设发现四卡的机器上显卡 0,1 使用中，显卡 2,3 空闲，Linux 终端输入:\n",
    "\n",
    "```\n",
    "export CUDA_VISIBLE_DEVICES=2,3\n",
    "```\n",
    "\n",
    "或在代码中加入\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"2,3\"\n",
    "```\n",
    "\n",
    "即可指定程序只在显卡 2,3 上运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置显存使用策略 \n",
    "\n",
    "默认情况下，TensorFlow 将使用几乎所有可用的显存，以避免内存碎片化所带来的性能损失。不过，TensorFlow 提供两种显存使用策略，让我们能够更灵活地控制程序的显存使用方式：\n",
    "\n",
    "- 仅在需要时申请显存空间（程序初始运行时消耗很少的显存，随着程序的运行而动态申请显存）；\n",
    "- 限制消耗固定大小的显存（程序不会超出限定的显存大小，若超出的报错）。\n",
    "\n",
    "可以通过 `tf.config.experimental.set_memory_growth` 将 GPU 的显存使用策略设置为 “仅在需要时申请显存空间”。以下代码将所有 GPU 设置为仅在需要时申请显存空间：\n",
    "\n",
    "```\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(device=gpu, True)\n",
    "```\n",
    "\n",
    "以下代码通过 `tf.config.experimental.set_virtual_device_configuration` 选项并传入 `tf.config.experimental.VirtualDeviceConfiguration` 实例，设置 TensorFlow 固定消耗 `GPU:0` 的 1GB 显存（其实可以理解为建立了一个显存大小为 1GB 的 “虚拟 GPU”）：\n",
    "\n",
    "```\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "```\n",
    "\n",
    "提示\n",
    "\n",
    "TensorFlow 1.X 的 Graph Execution 下，可以在实例化新的 session 时传入 `tf.compat.v1.ConfigPhoto` 类来设置 TensorFlow 使用显存的策略。具体方式是实例化一个 `tf.ConfigProto` 类，设置参数，并在创建 `tf.compat.v1.Session` 时指定 Config 参数。以下代码通过 `allow_growth` 选项设置 TensorFlow 仅在需要时申请显存空间：\n",
    "\n",
    "```\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "```\n",
    "\n",
    "以下代码通过 `per_process_gpu_memory_fraction` 选项设置 TensorFlow 固定消耗 40% 的 GPU 显存：\n",
    "\n",
    "```\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "tf.compat.v1.Session(config=config)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单 GPU 模拟多 GPU 环境 \n",
    "\n",
    "当我们的本地开发环境只有一个 GPU，但却需要编写多 GPU 的程序在工作站上进行训练任务时，TensorFlow 为我们提供了一个方便的功能，可以让我们在本地开发环境中建立多个模拟 GPU，从而让多 GPU 的程序调试变得更加方便。以下代码在实体 GPU `GPU:0` 的基础上建立了两个显存均为 2GB 的虚拟 GPU。\n",
    "\n",
    "```\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    gpus[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048),\n",
    "     tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "```\n",
    "\n",
    "我们在 [单机多卡训练](https://tf.wiki/zh/appendix/distributed.html#multi-gpu) 的代码前加入以上代码，即可让原本为多 GPU 设计的代码在单 GPU 环境下运行。当输出设备数量时，程序会输出：\n",
    "\n",
    "```\n",
    "Number of devices: 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Serving\n",
    "\n",
    "当我们将模型训练完毕后，往往需要将模型在生产环境中部署。最常见的方式，是在服务器上提供一个 API，即客户机向服务器的某个 API 发送特定格式的请求，服务器收到请求数据后通过模型进行计算，并返回结果。如果仅仅是做一个 Demo，不考虑高并发和性能问题，其实配合 [Flask](https://palletsprojects.com/p/flask/) 等 Python 下的 Web 框架就能非常轻松地实现服务器 API。不过，如果是在真的实际生产环境中部署，这样的方式就显得力不从心了。这时，TensorFlow 为我们提供了 TensorFlow Serving 这一组件，能够帮助我们在实际生产环境中灵活且高性能地部署机器学习模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Serving 安装 \n",
    "\n",
    "TensorFlow Serving 可以使用 apt-get 或 Docker 安装。在生产环境中，推荐 [使用 Docker 部署 TensorFlow Serving](https://www.tensorflow.org/tfx/serving/docker) 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Download the TensorFlow Serving Docker image and repo\n",
    "docker pull tensorflow/serving\n",
    "\n",
    "git clone https://github.com/tensorflow/serving\n",
    "# Location of demo models\n",
    "TESTDATA=\"$(pwd)/serving/tensorflow_serving/servables/tensorflow/testdata\"\n",
    "\n",
    "# Start TensorFlow Serving container and open the REST API port\n",
    "docker run -t --rm -p 8501:8501 \\\n",
    "    -v \"$TESTDATA/saved_model_half_plus_two_cpu:/models/half_plus_two\" \\\n",
    "    -e MODEL_NAME=half_plus_two \\\n",
    "    tensorflow/serving &\n",
    "\n",
    "# Query the model using the predict API\n",
    "curl -d '{\"instances\": [1.0, 2.0, 5.0]}' \\\n",
    "    -X POST http://localhost:8501/v1/models/half_plus_two:predict\n",
    "\n",
    "# Returns => { \"predictions\": [2.5, 3.0, 4.5] }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving with Docker\n",
    "\n",
    "### Pulling a serving image\n",
    "\n",
    "Once you have Docker installed, you can pull the latest TensorFlow Serving docker image by running:\n",
    "\n",
    "```shell\n",
    "docker pull tensorflow/serving\n",
    "```\n",
    "\n",
    "This will pull down an minimal Docker image with TensorFlow Serving installed.\n",
    "\n",
    "See the Docker Hub [tensorflow/serving repo](http://hub.docker.com/r/tensorflow/serving/tags/) for other versions of images you can pull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a serving image\n",
    "### The serving images (both CPU and GPU) have the following properties:\n",
    "\n",
    "Port 8500 exposed for gRPC\n",
    "Port 8501 exposed for the REST API\n",
    "Optional environment variable MODEL_NAME (defaults to model)\n",
    "Optional environment variable MODEL_BASE_PATH (defaults to /models)\n",
    "When the serving image runs ModelServer, it runs it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "  --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To serve with Docker, you'll need:\n",
    "\n",
    "- An open port on your host to serve on\n",
    "- A SavedModel to serve\n",
    "- A name for your model that your client will refer to\n",
    "\n",
    "What you'll do is [run the Docker](https://docs.docker.com/engine/reference/run/) container, [publish](https://docs.docker.com/engine/reference/commandline/run/#publish-or-expose-port--p---expose) the container's ports to your host's ports, and mounting your host's path to the SavedModel to where the container expects models.\n",
    "\n",
    "Let's look at an example:\n",
    "\n",
    "```shell\n",
    "docker run -p 8501:8501 \\\n",
    "  --mount type=bind,source=/path/to/my_model/,target=/models/my_model \\\n",
    "  -e MODEL_NAME=my_model -t tensorflow/serving\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在这种情况下，我们启动了一个Docker容器，将REST API端口8501发布到主机的端口8501，并获取了一个我们命名的模型`my_model`并将其绑定到默认的模型基本路径（`${MODEL_BASE_PATH}/${MODEL_NAME}`= `/models/my_model`）。最后，我们填补了环境变量 `MODEL_NAME`有`my_model`，离开`MODEL_BASE_PATH`它的默认值。\n",
    "\n",
    "这将在容器中运行："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```shell\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "  --model_name=my_model --model_base_path=/models/my_model\n",
    "```\n",
    "\n",
    "如果要发布gRPC端口，可以使用`-p 8500:8500`。您可以同时打开gRPC和REST API端口，或者选择仅打开一个端口。\n",
    "\n",
    "传递其他参数\n",
    "\n",
    "`tensorflow_model_server`支持许多其他参数，您可以将这些参数传递给服务的Docker容器。例如，如果我们要传递模型配置文件而不是指定模型名称，则可以执行以下操作：\n",
    "\n",
    "```shell\n",
    "docker run -p 8500:8500 -p 8501:8501 \\\n",
    "  --mount type=bind,source=/path/to/my_model/,target=/models/my_model \\\n",
    "  --mount type=bind,source=/path/to/my/models.config,target=/models/models.config \\\n",
    "  -t tensorflow/serving --model_config_file=/models/models.config\n",
    "```\n",
    "\n",
    "此方法适用于支持的任何其他命令行参数 `tensorflow_model_server`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

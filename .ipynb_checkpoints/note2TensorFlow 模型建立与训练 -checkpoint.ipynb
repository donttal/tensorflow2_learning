{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow 模型建立与训练 \n",
    "\n",
    "本章介绍如何使用 TensorFlow 快速搭建动态模型。\n",
    "\n",
    "- 模型的构建： `tf.keras.Model` 和 `tf.keras.layers`\n",
    "\n",
    "- 模型的损失函数： `tf.keras.losses`\n",
    "\n",
    "- 模型的优化器： `tf.keras.optimizer`\n",
    "\n",
    "- 模型的评估： `tf.keras.metrics`/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置知识\n",
    "\n",
    "- [Python 面向对象编程](http://www.runoob.com/python3/python3-class.html) （在 Python 内定义类和方法、类的继承、构造和析构函数，[使用 super () 函数调用父类方法](http://www.runoob.com/python/python-func-super.html) ，[使用__call__() 方法对实例进行调用](https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/0014319098638265527beb24f7840aa97de564ccc7f20f6000) 等）；\n",
    "- 多层感知机、卷积神经网络、循环神经网络和强化学习（每节之前给出参考资料）。\n",
    "- [Python 的函数装饰器](https://www.runoob.com/w3cnote/python-func-decorators.html) （非必须）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型（Model）与层（Layer）\n",
    "\n",
    "在 TensorFlow 中，推荐使用 Keras（ `tf.keras` ）构建模型。Keras 是一个广为流行的高级神经网络 API，简单、快速而不失灵活性，现已得到 TensorFlow 的官方内置和全面支持。\n",
    "\n",
    "Keras 有两个重要的概念： **模型（Model）** 和 **层（Layer）** 。层将各种计算流程和变量进行了封装（例如基本的全连接层，CNN 的卷积层、池化层等），而模型则将各种层进行组织和连接，并封装成一个整体，描述了如何将输入数据通过各种层以及运算而得到输出。在需要模型调用的时候，使用 `y_pred = model(X)` 的形式即可。Keras 在 `tf.keras.layers` 下内置了深度学习中大量常用的的预定义层，同时也允许我们自定义层。\n",
    "\n",
    "Keras 模型以类的形式呈现，我们可以通过继承 `tf.keras.Model` 这个 Python 类来定义自己的模型。在继承类中，我们需要重写 `__init__()` （构造函数，初始化）和 `call(input)` （模型调用）两个方法，同时也可以根据需要增加自定义的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()     # Python 2 下使用 super(MyModel, self).__init__()\n",
    "        # 此处添加初始化代码（包含 call 方法中会用到的层），例如\n",
    "        # layer1 = tf.keras.layers.BuiltInLayer(...)\n",
    "        # layer2 = MyCustomLayer(...)\n",
    "\n",
    "    def call(self, input):\n",
    "        # 此处添加模型调用的代码（处理输入并返回输出），例如\n",
    "        # x = layer1(input)\n",
    "        # output = layer2(x)\n",
    "        return output\n",
    "\n",
    "    # 还可以添加自定义的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![../../_images/model.png](https://tf.wiki/_images/model.png)](https://tf.wiki/_images/model.png)\n",
    "\n",
    "Keras 模型类定义示意图 \n",
    "\n",
    "继承 `tf.keras.Model` 后，我们同时可以使用父类的若干方法和属性，例如在实例化类 `model = Model()` 后，可以通过 `model.variables` 这一属性直接获得模型中的所有变量，免去我们一个个显式指定变量的麻烦。\n",
    "\n",
    "上一章中简单的线性模型 `y_pred = a * X + b` ，我们可以通过模型类的方式编写如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'linear_1/dense_2/kernel:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [nan]], dtype=float32)>, <tf.Variable 'linear_1/dense_2/bias:0' shape=(1,) dtype=float32, numpy=array([nan], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1., 2., 3.],[4., 5., 6.]])\n",
    "y = tf.constant([10.0, 20.0])\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Dense = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.zeros_initializer,\n",
    "            bias_initializer=tf.zeros_initializer,\n",
    "        )\n",
    "    def call(self, input):\n",
    "        out = self.Dense(input)\n",
    "        return out\n",
    "\n",
    "# 模型训练\n",
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(x)\n",
    "        loss = tf.reduce_sum(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars = zip(grads, model.variables))\n",
    "print(model.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们没有显式地声明 `a` 和 `b` 两个变量并写出 `y_pred = a * X + b` 这一线性变换，而是建立了一个继承了 `tf.keras.Model` 的模型类 `Linear` 。这个类在初始化部分实例化了一个 **全连接层** （ `tf.keras.layers.Dense` ），并在 call 方法中对这个层进行调用，实现了线性变换的计算。如果需要显式地声明自己的变量并使用变量进行自定义运算，或者希望了解 Keras 层的内部原理，请参考 [自定义层](https://tf.wiki/zh/basic/models.html#custom-layer)。\n",
    "\n",
    "Keras 的全连接层：线性变换 + 激活函数\n",
    "\n",
    "[全连接层](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) （Fully-connected Layer，`tf.keras.layers.Dense` ）是 Keras 中最基础和常用的层之一，对输入矩阵 ![A](https://tf.wiki/_images/math/211284f68205c3e66773eaf026f32a0acdd3dfb3.png) 进行 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 的线性变换 + 激活函数操作。如果不指定激活函数，即是纯粹的线性变换 ![AW + b](https://tf.wiki/_images/math/7195d0ad48bfa4fb60a47b82eb81b21d21ef9d9f.png)。具体而言，给定输入张量 `input = [batch_size, input_dim]` ，该层对输入张量首先进行 `tf.matmul(input, kernel) + bias` 的线性变换（ `kernel` 和 `bias` 是层中可训练的变量），然后对线性变换后张量的每个元素通过激活函数 `activation` ，从而输出形状为 `[batch_size, units]` 的二维张量。\n",
    "\n",
    "[![../../_images/dense.png](https://tf.wiki/_images/dense.png)](https://tf.wiki/_images/dense.png)\n",
    "\n",
    "其包含的主要参数如下：\n",
    "\n",
    "- `units` ：输出张量的维度；\n",
    "- `activation` ：激活函数，对应于 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 中的 ![f](https://tf.wiki/_images/math/5b7752c757e0b691a80ab8227eadb8a8389dc58a.png) ，默认为无激活函数（ `a(x) = x` ）。常用的激活函数包括 `tf.nn.relu` 、 `tf.nn.tanh` 和 `tf.nn.sigmoid` ；\n",
    "- `use_bias` ：是否加入偏置向量 `bias` ，即 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 中的 ![b](https://tf.wiki/_images/math/68c7c8c65602677ab56cf7fd88002023f0edc575.png)。默认为 `True` ；\n",
    "- `kernel_initializer` 、 `bias_initializer` ：权重矩阵 `kernel` 和偏置向量 `bias` 两个变量的初始化器。默认为 `tf.glorot_uniform_initializer` [1](https://tf.wiki/zh/basic/models.html#glorot) 。设置为 `tf.zeros_initializer` 表示将两个变量均初始化为全 0；\n",
    "\n",
    "该层包含权重矩阵 `kernel = [input_dim, units]` 和偏置向量 `bias = [units]` [2](https://tf.wiki/zh/basic/models.html#broadcast) 两个可训练变量，对应于 ![f(AW + b)](https://tf.wiki/_images/math/c324a7004aff2d31917fb70e1311c4f43449a14c.png) 中的 ![W](https://tf.wiki/_images/math/1fbee781f84569077719a167b64e12064360fac1.png) 和 ![b](https://tf.wiki/_images/math/68c7c8c65602677ab56cf7fd88002023f0edc575.png)。\n",
    "\n",
    "这里着重从数学矩阵运算和线性变换的角度描述了全连接层。基于神经元建模的描述可参考 [后文介绍](https://tf.wiki/zh/basic/models.html#neuron) 。\n",
    "\n",
    "- [1](https://tf.wiki/zh/basic/models.html#id3)\n",
    "\n",
    "  Keras 中的很多层都默认使用 `tf.glorot_uniform_initializer` 初始化变量，关于该初始化器可参考 https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer 。\n",
    "\n",
    "- [2](https://tf.wiki/zh/basic/models.html#id4)\n",
    "\n",
    "  你可能会注意到， `tf.matmul(input, kernel)` 的结果是一个形状为 `[batch_size, units]` 的二维矩阵，这个二维矩阵要如何与形状为 `[units]` 的一维偏置向量 `bias` 相加呢？事实上，这里是 TensorFlow 的 Broadcasting 机制在起作用，该加法运算相当于将二维矩阵的每一行加上了 `Bias` 。Broadcasting 机制的具体介绍可见 https://www.tensorflow.org/xla/broadcasting 。\n",
    "\n",
    "为什么模型类是重载 `call()` 方法而不是 `__call__()` 方法？\n",
    "\n",
    "在 Python 中，对类的实例 `myClass` 进行形如 `myClass()` 的调用等价于 `myClass.__call__()` （具体请见本章初 “前置知识” 的 `__call__()` 部分）。那么看起来，为了使用 `y_pred = model(X)` 的形式调用模型类，应该重写 `__call__()` 方法才对呀？原因是 Keras 在模型调用的前后还需要有一些自己的内部操作，所以暴露出一个专门用于重载的 `call()` 方法。 `tf.keras.Model` 这一父类已经包含 `__call__()` 的定义。 `__call__()` 中主要调用了 `call()` 方法，同时还需要在进行一些 keras 的内部操作。这里，我们通过继承 `tf.keras.Model` 并重载 `call()` 方法，即可在保持 keras 结构的同时加入模型调用的代码。\n",
    "\n",
    "\n",
    "\n",
    "## 基础示例：多层感知机（MLP）\n",
    "\n",
    "我们从编写一个最简单的 [多层感知机](https://zh.wikipedia.org/wiki/多层感知器) （Multilayer Perceptron, MLP），或者说 “多层全连接神经网络” 开始，介绍 TensorFlow 的模型编写方式。在这一部分，我们依次进行以下步骤：\n",
    "\n",
    "- 使用 `tf.keras.datasets` 获得数据集并预处理\n",
    "- 使用 `tf.keras.Model` 和 `tf.keras.layers` 构建模型\n",
    "- 构建模型训练流程，使用 `tf.keras.losses` 计算损失函数，并使用 `tf.keras.optimizer` 优化模型\n",
    "- 构建模型评估流程，使用 `tf.keras.metrics` 计算评估指标\n",
    "\n",
    "基础知识和原理\n",
    "\n",
    "- UFLDL 教程 [Multi-Layer Neural Network](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/) 一节；\n",
    "- 斯坦福课程 [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/) 中的 “Neural Networks Part 1 ~ 3” 部分。\n",
    "\n",
    "这里，我们使用多层感知机完成 MNIST 手写体数字图片数据集 [[LeCun1998\\]](https://tf.wiki/zh/basic/models.html#lecun1998) 的分类任务。\n",
    "\n",
    "![../../_images/mnist_0-9.png](https://tf.wiki/_images/mnist_0-9.png)\n",
    "\n",
    "MNIST 手写体数字图片示例 \n",
    "\n",
    "### 数据获取及预处理： `tf.keras.datasets`\n",
    "\n",
    "先进行预备工作，实现一个简单的 `MNISTLoader` 类来读取 MNIST 数据集数据。这里使用了 `tf.keras.datasets` 快速载入 MNIST 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据获取及预处理\n",
    "import numpy as np\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32)/255.0, axis=-1)\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32)/255.0, axis=-1)\n",
    "        self.train_label = self.train_label.astype(np.int32)\n",
    "        self.test_label = self.test_label.astype(np.int32)\n",
    "        self.num_train_data, self.num_test_data= self.train_data.shape[0], self.test_data.shape[0]\n",
    "    \n",
    "    def get_batch(self, batch_size):\n",
    "        index = np.randon.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mnist = tf.keras.datasets.mnist` 将从网络上自动下载 MNIST 数据集并加载。如果运行时出现网络连接错误，可以从 https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz 或 https://s3.amazonaws.com/img-datasets/mnist.npz 下载 MNIST 数据集 `mnist.npz` 文件，并放置于用户目录的 `.keras/dataset` 目录下（Windows 下用户目录为 `C:\\Users\\用户名` ，Linux 下用户目录为 `/home/用户名` ）。\n",
    "\n",
    "TensorFlow 的图像数据表示\n",
    "\n",
    "在 TensorFlow 中，图像数据集的一种典型表示是 `[图像数目，长，宽，色彩通道数]` 的四维张量。在上面的 `DataLoader` 类中， `self.train_data` 和 `self.test_data` 分别载入了 60,000 和 10,000 张大小为 `28*28` 的手写体数字图片。由于这里读入的是灰度图片，色彩通道数为 1（彩色 RGB 图像色彩通道数为 3），所以我们使用 `np.expand_dims()` 函数为图像数据手动在最后添加一维通道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的构建： `tf.keras.Model` 和 `tf.keras.layers`\n",
    "\n",
    "多层感知机的模型类实现与上面的线性模型类似，使用 `tf.keras.Model` 和 `tf.keras.layers` 构建，所不同的地方在于层数增加了（顾名思义，“多层” 感知机），以及引入了非线性激活函数（这里使用了 [ReLU 函数](https://zh.wikipedia.org/wiki/线性整流函数) ， 即下方的 `activation=tf.nn.relu` ）。该模型输入一个向量（比如这里是拉直的 `1×784` 手写体数字图片），输出 10 维的向量，分别代表这张图片属于 0 到 9 的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, input):\n",
    "        x = self.flatten(input)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        output =  tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax 函数\n",
    "\n",
    "这里，因为我们希望输出 “输入图片分别属于 0 到 9 的概率”，也就是一个 10 维的离散概率分布，所以我们希望这个 10 维向量至少满足两个条件：\n",
    "\n",
    "- 该向量中的每个元素均在 ![[0, 1]](https://tf.wiki/_images/math/8027137b3073a7f5ca4e45ba2d030dcff154eca4.png) 之间；\n",
    "- 该向量的所有元素之和为 1。\n",
    "\n",
    "为了使得模型的输出能始终满足这两个条件，我们使用 [Softmax 函数](https://zh.wikipedia.org/wiki/Softmax函数) （归一化指数函数， `tf.nn.softmax` ）对模型的原始输出进行归一化。其形式为 ![\\sigma(\\mathbf{z})_j = \\frac{e^{z_j}}{\\sum_{k=1}^K e^{z_k}}](https://tf.wiki/_images/math/7d714874b555007ada90b5315e5fa2ffa0e5e2ee.png) 。不仅如此，softmax 函数能够凸显原始向量中最大的值，并抑制远低于最大值的其他分量，这也是该函数被称作 softmax 函数的原因（即平滑化的 argmax 函数）。\n",
    "\n",
    "[![../../_images/mlp.png](https://tf.wiki/_images/mlp.png)](https://tf.wiki/_images/mlp.png)\n",
    "\n",
    "MLP 模型示意图 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的训练： `tf.keras.losses` 和 `tf.keras.optimizer`\n",
    "\n",
    "定义一些模型超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A local file was found, but it seems to be incomplete or outdated because the auto file hash does not match the original value of 8a61469f7ea1b51cbae51d4f78837e45 so we will re-download the data.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      " 4636672/11490434 [===========>..................] - ETA: 17:06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d1bf8faaa6b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-24b633d79c4a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/datasets/mnist.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m       \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morigin_folder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'mnist.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m       file_hash='8a61469f7ea1b51cbae51d4f78837e45')\n\u001b[0m\u001b[1;32m     50\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1050\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1052\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    909\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后迭代进行以下步骤：\n",
    "\n",
    "- 从 DataLoader 中随机取一批训练数据；\n",
    "- 将这批数据送入模型，计算出模型的预测值；\n",
    "- 将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 `tf.keras.losses` 中的交叉熵函数作为损失函数；\n",
    "- 计算损失函数关于模型变量的导数；\n",
    "- 将求出的导数值传入优化器，使用优化器的 `apply_gradients` 方法更新模型参数以最小化损失函数（优化器的详细使用方法见 [前章](https://tf.wiki/en/basic/basic.html#optimizer) ）。\n",
    "\n",
    "具体代码实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_batches)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.varialbes)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "交叉熵（cross entropy）与 `tf.keras.losses`\n",
    "\n",
    "你或许注意到了，在这里，我们没有显式地写出一个损失函数，而是使用了 `tf.keras.losses` 中的 `sparse_categorical_crossentropy` （交叉熵）函数，将模型的预测值 `y_pred` 与真实的标签值 `y` 作为函数参数传入，由 Keras 帮助我们计算损失函数的值。\n",
    "\n",
    "交叉熵作为损失函数，在分类问题中被广泛应用。其离散形式为 ![H(y, \\hat{y}) = -\\sum_{i=1}^{n}y_i \\log(\\hat{y_i})](https://tf.wiki/_images/math/63ad0688c80b4c83b2a6e0a542b741ed8f9ff79f.png) ，其中 ![y](https://tf.wiki/_images/math/1b5e577d6216dca3af7d87aa122a0b9b360d6cb3.png) 为真实概率分布， ![\\hat{y}](https://tf.wiki/_images/math/1257829df10bd602e03553570cadfe2328fd1d91.png) 为预测概率分布， ![n](https://tf.wiki/_images/math/5a939c5280da7202ca4531f175a7780ad5e1f80a.png) 为分类任务的类别个数。预测概率分布与真实分布越接近，则交叉熵的值越小，反之则越大。更具体的介绍及其在机器学习中的应用可参考 [这篇博客文章](https://blog.csdn.net/tsyccnh/article/details/79163834) 。\n",
    "\n",
    "在 `tf.keras` 中，有两个交叉熵相关的损失函数 `tf.keras.losses.categorical_crossentropy` 和 `tf.keras.losses.sparse_categorical_crossentropy` 。其中 sparse 的含义是，真实的标签值 `y_true` 可以直接传入 int 类型的标签类别。具体而言：\n",
    "\n",
    "```\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "```\n",
    "\n",
    "与\n",
    "\n",
    "```\n",
    "loss = tf.keras.losses.categorical_crossentropy(\n",
    "    y_true=tf.one_hot(y, depth=tf.shape(y_pred)[-1]),\n",
    "    y_pred=y_pred\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    num_batches = int(data_loader.num_test_data // batch_size)\n",
    "    for batch_index in range(num_batches):\n",
    "        start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "        y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "        sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "    print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络的基本单位：神经元 [3](https://tf.wiki/zh/basic/models.html#order)\n",
    "\n",
    "如果我们将上面的神经网络放大来看，详细研究计算过程，比如取第二层的第 k 个计算单元，可以得到示意图如下：\n",
    "\n",
    "[![../../_images/neuron.png](https://tf.wiki/_images/neuron.png)](https://tf.wiki/_images/neuron.png)\n",
    "\n",
    "该计算单元 ![Q_k](https://tf.wiki/_images/math/f9ea3fce3df86fede704cd6ad3f1d66d1a8e50da.png) 有 100 个权值参数 ![w_{0k}, w_{1k}, ..., w_{99k}](https://tf.wiki/_images/math/2b0aab8ff93cca2b0a16db9629dde29487dfc1b7.png) 和 1 个偏置参数 ![b_k](https://tf.wiki/_images/math/a83804f0832f64289f1399e2617515baa1850ac8.png) 。将第 1 层中所有的 100 个计算单元 ![P_0, P_1, ..., P_{99}](https://tf.wiki/_images/math/d7ce55802b5f2bdb503ac4f5dcb27445ae119043.png) 的值作为输入，分别按权值 ![w_{ik}](https://tf.wiki/_images/math/90c43dba16784b3690fc6c95be25c1fff7051b92.png) 加和（即 ![\\sum_{i=0}^{99} w_{ik} P_i](https://tf.wiki/_images/math/a21ecd55aa919e4228bdd3465665ba26a251f284.png) ），并加上偏置值 ![b_k](https://tf.wiki/_images/math/a83804f0832f64289f1399e2617515baa1850ac8.png) ，然后送入激活函数 ![f](https://tf.wiki/_images/math/5b7752c757e0b691a80ab8227eadb8a8389dc58a.png) 进行计算，即得到输出结果。\n",
    "\n",
    "事实上，这种结构和真实的神经细胞（神经元）类似。神经元由树突、胞体和轴突构成。树突接受其他神经元传来的信号作为输入（一个神经元可以有数千甚至上万树突），胞体对电位信号进行整合，而产生的信号则通过轴突传到神经末梢的突触，传播到下一个（或多个）神经元。\n",
    "\n",
    "[![../../_images/real_neuron.png](https://tf.wiki/_images/real_neuron.png)](https://tf.wiki/_images/real_neuron.png)\n",
    "\n",
    "神经细胞模式图（修改自 Quasar Jarosz at English Wikipedia [CC BY-SA 3.0 (https://creativecommons.org/licenses/by-sa/3.0)]）\n",
    "\n",
    "上面的计算单元，可以被视作对神经元结构的数学建模。在上面的例子里，第二层的每一个计算单元（人工神经元）有 100 个权值参数和 1 个偏置参数，而第二层计算单元的数目是 10 个，因此这一个全连接层的总参数量为 100*10 个权值参数和 10 个偏置参数。事实上，这正是该全连接层中的两个变量 `kernel` 和 `bias` 的形状。仔细研究一下，你会发现，这里基于神经元建模的介绍与上文基于矩阵计算的介绍是等价的。\n",
    "\n",
    "- [3](https://tf.wiki/zh/basic/models.html#id9)\n",
    "\n",
    "  事实上，应当是先有神经元建模的概念，再有基于人工神经元和层结构的人工神经网络。但由于本手册着重介绍 TensorFlow 的使用方法，所以调换了介绍顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

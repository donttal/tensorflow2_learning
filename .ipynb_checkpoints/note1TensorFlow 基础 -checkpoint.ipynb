{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前置知识\n",
    "- [Python 基本操作](http://www.runoob.com/python3/python3-tutorial.html) （赋值、分支及循环语句、使用 import 导入库）；\n",
    "- [Python 的 With 语句](https://www.ibm.com/developerworks/cn/opensource/os-cn-pythonwith/index.html) ；\n",
    "- [NumPy](https://docs.scipy.org/doc/numpy/user/quickstart.html) ，Python 下常用的科学计算库。TensorFlow 与之结合紧密；\n",
    "- [向量](https://zh.wikipedia.org/wiki/向量) 和 [矩阵](https://zh.wikipedia.org/wiki/矩阵) 运算（矩阵的加减法、矩阵与向量相乘、矩阵与矩阵相乘、矩阵的转置等。测试题：![\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 5 & 6 \\\\ 7 & 8 \\end{bmatrix} = ?](https://tf.wiki/_images/math/b480b0a427e86cd07160321578c23a8f30222019.png)）；\n",
    "- [函数的导数](http://old.pep.com.cn/gzsx/jszx_1/czsxtbjxzy/qrzptgjzxjc/dzkb/dscl/) ，[多元函数求导](https://zh.wikipedia.org/wiki/偏导数) （测试题：![f(x, y) = x^2 + xy + y^2, \\frac{\\partial f}{\\partial x} = ?, \\frac{\\partial f}{\\partial y} = ?](https://tf.wiki/_images/math/902db65185d3560d091fe47fdbf1fb561f082a76.png)）；\n",
    "- [线性回归](http://old.pep.com.cn/gzsx/jszx_1/czsxtbjxzy/qrzptgjzxjc/dzkb/dscl/) ；\n",
    "- [梯度下降方法](https://zh.wikipedia.org/wiki/梯度下降法) 求函数的局部最小值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:19:17.287586Z",
     "start_time": "2019-10-16T10:19:13.045554Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量\n",
    "TensorFlow 使用 张量 （Tensor）作为数据的基本单位。TensorFlow 的张量在概念上等同于多维数组，我们可以使用它来描述数学中的标量（0 维数组）、向量（1 维数组）、矩阵（2 维数组）等各种量，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:24:12.779660Z",
     "start_time": "2019-10-16T10:24:12.756681Z"
    },
    "cell_style": "center",
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# 定义一个随机数（标量）\n",
    "random_float = tf.random.uniform(shape=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:28:10.340473Z",
     "start_time": "2019-10-16T10:28:10.337798Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义一个有2个元素的零向量\n",
    "zeros_vector = tf.zeros(shape=(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:36:02.971214Z",
     "start_time": "2019-10-16T10:36:02.967943Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义两个2×2的常量矩阵\n",
    "A = tf.constant([[1.,2.],[3.,4.]])\n",
    "B = tf.constant([[5.,6.],[7.,8.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 的大多数 API 函数会根据输入的值自动推断张量中元素的类型（一般默认为 tf.float32 ）。不过你也可以通过加入 dtype 参数来自行指定类型，例如 zero_vector = tf.zeros(shape=(2), dtype=tf.int32) 将使得张量中的元素类型均为整数。张量的 numpy() 方法是将张量的值转换为一个 NumPy 数组。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:36:03.260581Z",
     "start_time": "2019-10-16T10:36:03.256863Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "<dtype: 'float32'>\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# 查看矩阵A的形状、类型和值\n",
    "print(A.shape)      # 输出(2, 2)，即矩阵的长和宽均为2\n",
    "print(A.dtype)      # 输出<dtype: 'float32'>\n",
    "print(A.numpy())    # 输出[[1. 2.]\n",
    "                    #      [3. 4.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:50:12.833295Z",
     "start_time": "2019-10-16T10:50:12.828834Z"
    }
   },
   "outputs": [],
   "source": [
    "C = tf.add(A, B)\n",
    "D = tf.matmul(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导机制 \n",
    "\n",
    "在机器学习中，我们经常需要计算函数的导数。TensorFlow 提供了强大的 **自动求导机制** 来计算导数。以下代码展示了如何使用 `tf.GradientTape()` 计算函数 ![y(x) = x^2](https://tf.wiki/_images/math/4e8a9c0003b5511a6cedaafcbb0075edf9089443.png) 在 ![x = 3](https://tf.wiki/_images/math/8153c976979a8a4fe01919cd83dda44e9e81f769.png) 时的导数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:53:28.482399Z",
     "start_time": "2019-10-16T10:53:28.475993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=55, shape=(), dtype=float32, numpy=9.0>, <tf.Tensor: id=59, shape=(), dtype=float32, numpy=6.0>]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(initial_value=3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.square(x)\n",
    "y_grad = tape.gradient(y, x)\n",
    "print([y, y_grad])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.GradientTape()` 是一个自动求导的记录器，在其中的变量和计算步骤都会被自动记录。在上面的示例中，变量 `x` 和计算步骤 `y = tf.square(x)` 被自动记录，因此可以通过 `y_grad = tape.gradient(y, x)` 求张量 `y` 对变量 `x` 的导数。\n",
    "\n",
    "在机器学习中，更加常见的是对多元函数求偏导数，以及对向量或矩阵的求导。这些对于 TensorFlow 也不在话下。以下代码展示了如何使用 `tf.GradientTape()` 计算函数 ![L(w, b) = \\|Xw + b - y\\|^2](https://tf.wiki/_images/math/bc1111827c1a7e4977fba4f9ff1c28062b80fefe.png) 在 ![w = (1, 2)^T, b = 1](https://tf.wiki/_images/math/d89c25edfe26bc3a38e198e6a69f47d1e0c964ad.png) 时分别对 ![w, b](https://tf.wiki/_images/math/d982c1679f77f34dd991ceb8f8696a24b1f83072.png) 的偏导数。其中 ![X = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix},  y = \\begin{bmatrix} 1 \\\\ 2\\end{bmatrix}](https://tf.wiki/_images/math/cdde5d2adfb12f753c17b723d04e0708298df0d0.png)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:55:16.225226Z",
     "start_time": "2019-10-16T10:55:16.211946Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62.5, array([[35.],\n",
      "       [50.]], dtype=float32), 15.0]\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant([[1., 2.], [3., 4.]])\n",
    "y = tf.constant([[1.], [2.]])\n",
    "w = tf.Variable(initial_value=[[1.], [2.]])\n",
    "b = tf.Variable(initial_value=1.)\n",
    "with tf.GradientTape() as tape:\n",
    "    L = 0.5 * tf.reduce_sum(tf.square(tf.matmul(X, w) + b - y))\n",
    "w_grad, b_grad = tape.gradient(L, [w, b])        # 计算L(w, b)关于w, b的偏导数\n",
    "print([L.numpy(), w_grad.numpy(), b_grad.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里， `tf.square()` 操作代表对输入张量的每一个元素求平方，不改变张量形状。 `tf.reduce_sum()` 操作代表对输入张量的所有元素求和，输出一个形状为空的纯量张量（可以通过 `axis` 参数来指定求和的维度，不指定则默认对所有元素求和）。TensorFlow 中有大量的张量操作 API，包括数学运算、张量形状操作（如 `tf.reshape()`）、切片和连接（如 `tf.concat()`）等多种类型，可以通过查阅 TensorFlow 的官方 API 文档 [2](https://tf.wiki/zh/basic/basic.html#f3) 来进一步了解。\n",
    "从输出可见，TensorFlow 帮助我们计算出了\n",
    "\n",
    "![L((1, 2)^T, 1) &= 62.5  \\frac{\\partial L(w, b)}{\\partial w} |_{w = (1, 2)^T, b = 1} &= \\begin{bmatrix} 35 \\\\ 50\\end{bmatrix}  \\frac{\\partial L(w, b)}{\\partial b} |_{w = (1, 2)^T, b = 1} &= 15](https://tf.wiki/_images/math/fa385fac979f9a8a42b02cbed89f2e3e2b2ca14b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:57:40.517212Z",
     "start_time": "2019-10-16T10:57:40.511452Z"
    }
   },
   "source": [
    "## 线性回归\n",
    "\n",
    "基础知识和原理\n",
    "\n",
    "- UFLDL 教程 [Linear Regression](http://ufldl.stanford.edu/tutorial/supervised/LinearRegression/) 一节。\n",
    "\n",
    "考虑一个实际问题，某城市在 2013 年 - 2017 年的房价如下表所示：\n",
    "\n",
    "| 年份 | 2013  | 2014  | 2015  | 2016  | 2017  |\n",
    "| ---- | ----- | ----- | ----- | ----- | ----- |\n",
    "| 房价 | 12000 | 14000 | 15000 | 16500 | 17500 |\n",
    "\n",
    "现在，我们希望通过对该数据进行线性回归，即使用线性模型 ![y = ax + b](https://tf.wiki/_images/math/4b73dd4430869b9fd6bcb231d4cc119b153f5d10.png) 来拟合上述数据，此处 `a` 和 `b` 是待求的参数。\n",
    "\n",
    "首先，我们定义数据，进行基本的归一化操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T10:57:08.550020Z",
     "start_time": "2019-10-16T10:57:08.544916Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_raw = np.array([2013, 2014, 2015, 2016, 2017], dtype=np.float32)\n",
    "y_raw = np.array([12000, 14000, 15000, 16500, 17500], dtype=np.float32)\n",
    "# 归一化操作\n",
    "X = (X_raw - X_raw.min()) / (X_raw.max() - X_raw.min())\n",
    "y = (y_raw - y_raw.min()) / (y_raw.max() - y_raw.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾机器学习的基础知识，对于多元函数 ![f(x)](https://tf.wiki/_images/math/95aabe3d9002fe03d54b9b9d9fbfc27c0eaeb56f.png) 求局部极小值，[梯度下降](https://zh.wikipedia.org/wiki/梯度下降法) 的过程如下：\n",
    "\n",
    "- 初始化自变量为 ![x_0](https://tf.wiki/_images/math/ed7fb0260e58d3ca5851e823ff991dae4cde5671.png) ， ![k=0](https://tf.wiki/_images/math/7b22543df34ec7a2c80b915646591f6cc5eb31f8.png)\n",
    "\n",
    "- 迭代进行下列步骤直到满足收敛条件：\n",
    "\n",
    "  > - 求函数 ![f(x)](https://tf.wiki/_images/math/95aabe3d9002fe03d54b9b9d9fbfc27c0eaeb56f.png) 关于自变量的梯度 ![\\nabla f(x_k)](https://tf.wiki/_images/math/73b95e346989ee4b13399239a32f9e7463e377d9.png)\n",
    "  > - 更新自变量： ![x_{k+1} = x_{k} - \\gamma \\nabla f(x_k)](https://tf.wiki/_images/math/9a588336e13e66832cae445d20d75d3134dc8b89.png) 。这里 ![\\gamma](https://tf.wiki/_images/math/34d137cf01c787ecda732761c3f95b0f65a6c3e9.png) 是学习率（也就是梯度下降一次迈出的 “步子” 大小）\n",
    "  > - ![k \\leftarrow k+1](https://tf.wiki/_images/math/7a323dca731267c98cff9fe8afdf6ec1e699b223.png)\n",
    "\n",
    "接下来，我们考虑如何使用程序来实现梯度下降方法，求得线性回归的解 ![\\min_{a, b} L(a, b) = \\sum_{i=1}^n(ax_i + b - y_i)^2](https://tf.wiki/_images/math/3d80f73898766e92897c82400278aaf4c60cf373.png) 。\n",
    "\n",
    "### NumPy 下的线性回归 \n",
    "\n",
    "机器学习模型的实现并不是 TensorFlow 的专利。事实上，对于简单的模型，即使使用常规的科学计算库或者工具也可以求解。在这里，我们使用 NumPy 这一通用的科学计算库来实现梯度下降方法。NumPy 提供了多维数组支持，可以表示向量、矩阵以及更高维的张量。同时，也提供了大量支持在多维数组上进行操作的函数（比如下面的 `np.dot()` 是求内积， `np.sum()` 是求和）。在这方面，NumPy 和 MATLAB 比较类似。在以下代码中，我们手工求损失函数关于参数 `a` 和 `b` 的偏导数 [4](https://tf.wiki/zh/basic/basic.html#f2)，并使用梯度下降法反复迭代，最终获得 `a` 和 `b` 的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T11:21:27.341464Z",
     "start_time": "2019-10-16T11:21:27.188124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763702027872221 0.057564988311377796\n"
     ]
    }
   ],
   "source": [
    "a, b = 0, 0\n",
    "\n",
    "num_epoch = 10000\n",
    "learning_rate = 1e-3\n",
    "for e in range(num_epoch):\n",
    "    # 手动计算损失函数关于自变量（模型参数）的梯度\n",
    "    y_pred = a * X + b\n",
    "    grad_a, grad_b = (y_pred - y).dot(X), (y_pred - y).sum()\n",
    "\n",
    "    # 更新参数\n",
    "    a, b = a - learning_rate * grad_a, b - learning_rate * grad_b\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow 下的线性回归 \n",
    "\n",
    "TensorFlow 的 **Eager Execution（动态图）模式** [5](https://tf.wiki/zh/basic/basic.html#f4) 与上述 NumPy 的运行方式十分类似，然而提供了更快速的运算（GPU 支持）、自动求导、优化器等一系列对深度学习非常重要的功能。以下展示了如何使用 TensorFlow 计算线性回归。可以注意到，程序的结构和前述 NumPy 的实现非常类似。这里，TensorFlow 帮助我们做了两件重要的工作：\n",
    "\n",
    "- 使用 `tape.gradient(ys, xs)` 自动计算梯度；\n",
    "- 使用 `optimizer.apply_gradients(grads_and_vars)` 自动更新模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-16T12:11:47.367937Z",
     "start_time": "2019-10-16T12:11:33.946837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.97637> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.057565063>\n"
     ]
    }
   ],
   "source": [
    "X = tf.constant(X)\n",
    "y = tf.constant(y)\n",
    "\n",
    "a = tf.Variable(initial_value=0.)\n",
    "b = tf.Variable(initial_value=0.)\n",
    "variables = [a, b]\n",
    "\n",
    "num_epoch = 10000\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "for e in range(num_epoch):\n",
    "    # 使用tf.GradientTape()记录损失函数的梯度信息\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = a * X + b\n",
    "        loss = 0.5 * tf.reduce_sum(tf.square(y_pred - y))\n",
    "    # TensorFlow自动计算损失函数关于自变量（模型参数）的梯度\n",
    "    grads = tape.gradient(loss, variables)\n",
    "    # TensorFlow自动根据梯度更新参数\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, variables))\n",
    "\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在这里，我们使用了前文的方式计算了损失函数关于参数的偏导数。同时，使用 `tf.keras.optimizers.SGD(learning_rate=1e-3)` 声明了一个梯度下降 **优化器** （Optimizer），其学习率为 1e-3。优化器可以帮助我们根据计算出的求导结果更新模型参数，从而最小化某个特定的损失函数，具体使用方式是调用其 `apply_gradients()` 方法。\n",
    "\n",
    "注意到这里，更新模型参数的方法 `optimizer.apply_gradients()` 需要提供参数 `grads_and_vars`，即待更新的变量（如上述代码中的 `variables` ）及损失函数关于这些变量的偏导数（如上述代码中的 `grads` ）。具体而言，这里需要传入一个 Python 列表（List），列表中的每个元素是一个 `（变量的偏导数，变量）` 对。比如这里是 `[(grad_a, a), (grad_b, b)]` 。我们通过 `grads = tape.gradient(loss, variables)` 求出 tape 中记录的 `loss` 关于 `variables = [a, b]` 中每个变量的偏导数，也就是 `grads = [grad_a, grad_b]`，再使用 Python 的 `zip()` 函数将 `grads = [grad_a, grad_b]` 和 `variables = [a, b]` 拼装在一起，就可以组合出所需的参数了。\n",
    "\n",
    "Python 的 `zip()` 函数\n",
    "\n",
    "`zip()` 函数是 Python 的内置函数。用自然语言描述这个函数的功能很绕口，但如果举个例子就很容易理解了：如果 `a = [1, 3, 5]`， `b = [2, 4, 6]`，那么 `zip(a, b) = [(1, 2), (3, 4), ..., (5, 6)]` 。即 “将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表”。在 Python 3 中， `zip()` 函数返回的是一个 zip 对象，本质上是一个生成器，需要调用 `list()` 来将生成器转换成列表。\n",
    "\n",
    "[![../../_images/zip.jpg](https://tf.wiki/_images/zip.jpg)](https://tf.wiki/_images/zip.jpg)\n",
    "\n",
    "Python 的 `zip()` 函数图示 \n",
    "\n",
    "在实际应用中，我们编写的模型往往比这里一行就能写完的线性模型 `y_pred = a * X + b` （模型参数为 `variables = [a, b]` ）要复杂得多。所以，我们往往会编写并实例化一个模型类 `model = Model()` ，然后使用 `y_pred = model(X)` 调用模型，使用 `model.variables` 获取模型参数。关于模型类的编写方式可见 [“TensorFlow 模型” 一章](https://tf.wiki/zh/basic/models.html)。\n",
    "\n",
    "- [1](https://tf.wiki/zh/basic/basic.html#id8)\n",
    "\n",
    "  Python 中可以使用整数后加小数点表示将该整数定义为浮点数类型。例如 `3.` 代表浮点数 `3.0`。\n",
    "\n",
    "- [2](https://tf.wiki/zh/basic/basic.html#id9)\n",
    "\n",
    "  主要可以参考 [Tensor Transformations](https://www.tensorflow.org/versions/r1.9/api_guides/python/array_ops) 和 [Math](https://www.tensorflow.org/versions/r1.9/api_guides/python/math_ops) 两个页面。可以注意到，TensorFlow 的张量操作 API 在形式上和 Python 下流行的科学计算库 NumPy 非常类似，如果对后者有所了解的话可以快速上手。\n",
    "\n",
    "- [3](https://tf.wiki/zh/basic/basic.html#id12)\n",
    "\n",
    "  其实线性回归是有解析解的。这里使用梯度下降方法只是为了展示 TensorFlow 的运作方式。\n",
    "\n",
    "- [4](https://tf.wiki/zh/basic/basic.html#id15)\n",
    "\n",
    "  此处的损失函数为均方差 ![L(x) = \\frac{1}{2} \\sum_{i=1}^5 (ax_i + b - y_i)^2](https://tf.wiki/_images/math/0bc351bdadeec4a1c3fd5c4be10715182daaf528.png)。其关于参数 `a` 和 `b` 的偏导数为 ![\\frac{\\partial L}{\\partial a} = \\sum_{i=1}^5 (ax_i + b - y) x_i](https://tf.wiki/_images/math/88db165663835c22d80d377dd4fb4026f321f9cc.png)，![\\frac{\\partial L}{\\partial b} = \\sum_{i=1}^5 (ax_i + b - y)](https://tf.wiki/_images/math/82027e450db611dc25d2918eb05c312b38bae6a0.png)\n",
    "\n",
    "- [5](https://tf.wiki/zh/basic/basic.html#id17)\n",
    "\n",
    "  与 Eager Execution 相对的是 Graph Execution（静态图）模式，即 TensorFlow 在 2018 年 3 月的 1.8 版本发布之前所主要使用的模式。本手册以面向快速迭代开发的动态模式为主，但会在附录中介绍静态图模式的基本使用，供需要的读者查阅。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
